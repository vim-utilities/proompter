*proompter.txt*      For Vim version 9.1       Last change: 2024 Oct 02


                            Proompter    by S0AndS0


Proompter                                                          *proompter*

1. Usage                                                     |proompter-usage|
2. Configuration                                     |proompter-configuration|
2.0. Configuration examples                 |proompter-configuration-examples|
2.1. Configuration explanations         |proompter-configuration-explanations|
3. Functions                                             |proompter-functions|
4. Notes                                                     |proompter-notes|
5. Common Errors                                            |proompter-errors|

==============================================================================
1. Usage                                                     *proompter-usage*

==============================================================================
2.0. Configuration examples                 *proompter-configuration-examples*

                Default configurations currently are; >
  let s:defaults = {
        \   'select': {
        \     'model_name': 'codellama',
        \   },
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/chat',
        \     'prompt_callbacks': {
        \       'chat': {},
        \       'generate': {},
        \     },
        \   },
        \   'channel': {
        \     'address': '127.0.0.1:11435',
        \     'options': {
        \       'mode': 'raw',
        \       'callback': v:null,
        \     },
        \   },
        \   'models': {
        \     'codellama': {
        \       'prompt_callbacks': {
        \         'chat': {},
        \         'generate': {},
        \       },
        \       'data': {
        \         'raw': v:false,
        \         'stream': v:true,
        \       },
        \     },
        \   },
        \ }
<
                ...  Please define your customizations to load before this
                script to have defaults defaulted and customization correctly
                overwrite configurations.

                Example of defining default callback functions for all models; >
  let g:proompter = {
        \   'select': {
        \     'model_name': 'codellama',
        \   },
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/chat',
        \     'prompt_callbacks': {
        \       'chat': {
        \         'preamble': { configurations, state ->
        \           proompter#callback#prompt#chat#Preamble({
        \             'configurations': configurations,
        \             'state': state,
        \           })
        \         },
        \         'context': { configurations, state ->
        \           proompter#callback#prompt#chat#Context({
        \             'configurations': configurations,
        \             'state': state,
        \             'context_size': 5,
        \           })
        \         },
        \         'input': function('proompter#callback#prompt#chat#Input'),
        \         'post': { prompt_callbacks_data, configurations, state ->
        \           proompter#callback#prompt#chat#Post({
        \             'data': prompt_callbacks_data,
        \             'configurations': configurations,
        \             'state': state,
        \             'out_bufnr': v:null,
        \           })
        \         },
        \       },
        \       'generate': {
        \         'preamble': { configurations, state ->
        \           proompter#callback#prompt#generate#Preamble({
        \             'configurations': configurations,
        \             'state': state,
        \             'filetype': 'javascript',
        \             'history_tags': { 'start': '<HISTORY>', 'end': '</HISTORY>'},
        \             'input_tags': { 'start': '<PROOMPT>', 'end': '</PROOMPT>'},
        \             'response_tags': { 'start': '<RESPONSE>', 'end': '</RESPONSE>'},
        \           })
        \         },
        \         'context': { configurations, state ->
        \           proompter#callback#prompt#generate#Context({
        \             'configurations': configurations,
        \             'state': state,
        \             'context_size': 5,
        \             'history_tags': { 'start': '<HISTORY>', 'end': '</HISTORY>'},
        \             'input_tags': { 'start': '<PROOMPT>', 'end': '</PROOMPT>'},
        \             'response_tags': { 'start': '<RESPONSE>', 'end': '</RESPONSE>'},
        \           })
        \         },
        \         'input': { value, configurations, _state ->
        \           proompter#callback#prompt#generate#Input({
        \             'value': value,
        \             'configurations': configurations,
        \             'input_tags': { 'start': '<PROOMPT>', 'end': '</PROOMPT>'},
        \           })
        \         },
        \         'post': { prompt_callbacks_data, configurations, _state ->
        \           proompter#callback#prompt#generate#Post({
        \             'data': prompt_callbacks_data,
        \             'configurations': configurations,
        \             'out_bufnr': v:null,
        \           })
        \         },
        \       },
        \     },
        \   },
        \   'channel': {
        \     'address': '127.0.0.1:11435',
        \     'options': {
        \       'mode': 'raw',
        \       'callback': { _channel_response, api_response ->
        \         proompter#callback#channel#StreamToBuffer(
        \           api_response,
        \           g:proompter,
        \           g:proompter_state,
        \           v:null,
        \         )
        \       },
        \     },
        \   },
        \   'models': {
        \     'codellama': {
        \       'prompt_callbacks': {
        \         'chat': {},
        \         'generate': {},
        \       },
        \       'data': {
        \         'prompt': '',
        \         'raw': v:false,
        \         'stream': v:true,
        \       },
        \     },
        \   },
        \ }
<
2.1. Configuration explanations         *proompter-configuration-explanations*

                                              *proompter-configuration-select*
{dictionary} with `model_name` {string} defined that points into the
`g:proompter.models` dictionary.  Example: >
  let g:proompter = {
        \   'select': {
        \     'model_name': 'codellama',
        \   },
        \   'models': {
        \     'codellama': {
        \       '...': '...'
        \     },
        \   },
        \ }

<
                                                 *proompter-configuration-api*
{dictionary} with `url` {string} defined that Ollama API is expected to listen
on for `/generate` or `/chat` endpoint requests.  Examples: >
  let g:proompter = {
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/chat',
        \   },
        \ }

  let g:proompter = {
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/generate',
        \   },
        \ }
<
You may also define default callbacks under `g:proompter.api` name-space,
per-endpoint, for parsing prompts into an expected format, ex. >
  let g:proompter = {
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/generate',
        \     'prompt_callbacks': {
        \       'chat': { '...': '...' },
        \       'generate': { '...': '...' },
        \     },
        \   },
        \ }
<
...  These may later be overridden on a per-model basis within the
`g:proompter.models[model_name].prompt_callbacks` data-structure.  Review the
|proompter-configuration-models-callbacks| sections for more details.

                                             *proompter-configuration-channel*
{dictionary} with `address` {string} defined that `proompter-channel-proxy.py`
is expected to listen for Vim channel requests, as well as `options`
dictionary that are passed to |ch_open()|. >
  {
    'channel': {
      'address': '127.0.0.1:11435',
      'options': {
        'mode': '<json|js|nl|raw|lsp>',
        'callback': <Funcref>,
      },
    },
  }

Note: this plugin is only tested with `channel.options.mode == 'raw'`, because
this allows for crafting completely custom HTTP POST request content.

Example: >
  let g:proompter = {
        \   'channel': {
        \     'address': '127.0.0.1:11435',
        \     'options': {
        \       'mode': 'raw',
        \       'callback': { _channel_response, api_response ->
        \         proompter#callback#channel#StreamToBuffer(
        \           api_response,
        \           g:proompter,
        \           g:proompter_state,
        \           v:null,
        \         )
        \       },
        \     },
        \   },
        \ }
<
Note: to simulate asynchronous behaviors this plugin requires a |channel|
proxy because |channel-address| does not allow for defining a path, and Ollama
API currently requires targeting a URL path to invoke various features.

                                              *proompter-configuration-models*
{dictionary} of dictionaries, with a structure similar to; >
  {
    '<g:proompter.select.model_name>': {
      'prompt_callbacks': {
        'preamble': <Funcref>,
        'context': <Funcref>,
        'input': <Funcref>,
        'post': <Funcref>,
      },
      'data': {
        'prompt': '<string>',
        'raw': <v:false|v:true>,
        'stream': <v:false|v:true>,
      },
    },
  }
<
Note: provided you use functions provided this plugin is _mostly_ compatible
with `data.stream` set to either `v:true` or `v:false`.  And setting
`data.raw` to `v:true` is currently untested.

                                    *proompter-configuration-models-callbacks*
Each callback should return a {string} when `g:proompter.api.url` endpoint is
`/generate`, or a {list} of dictionaries with `role` and `content` keys
defined with strings when endpoint is `/chat`.  Review the following links for
Ollama API expectations;

  - https://github.com/ollama/ollama/blob/main/docs/api.md#parameters
  - https://github.com/ollama/ollama/blob/main/docs/api.md#parameters-1

TLDR
  - `/api/chat` >
  [
    {
      'role': 'system',
      'content': 'Pretend I am your pirate captain, you are my crew, say "Aye Aye" instead of "yes"!',
    },
    {
      'role': 'user',
      'content': 'You must write a swash-buckling haiku about Vim being the best text editor, or walk the plank!',
    },
  ]
<
  - `/api/generate` >
  <PRE> def compute_gcd(x, y): <SUF>return result <MID>
<
                    *proompter-configuration-models-prompt-callbacks-preamble*
Default `preamble` callback is provided {dictionary} references to
`g:proompter`, and `g:proompter_state`, as first two parameters.  And should
return a {string} or dictionary {list}, depending on API endpoint, as
described in |proompter-configuration-models-callbacks| section.

Intended use-case is to _prime_ LLM with behavior constraints, context about
file and/or topic.  Example: >
  function! Preamble(configurations, _state) abort
    let l:endpoint = split(a:configurations.api.url, '/')[-1]
    let l:endpoint = split(l:endpoint, '?')[0]
    if l:endpoint == 'chat'
      return [
            \   {
            \     'role': 'system',
            \     'content': 'You are an expert with ' . &filetype
            \   },
            \ ]
    elseif l:endpoint == 'generate'
      return 'You are an expert with ' . &filetype
    else
      throw 'Unknown endpoint ->' . a:configurations.api.url
    endif
  endfunction
<
                     *proompter-configuration-models-prompt-callbacks-context*
Default `context` callback is provided {dictionary} references to
`g:proompter`, and `g:proompter_state`, as first two parameters.  And should
return a {string} or dictionary {list}, depending on API endpoint, as
described in |proompter-configuration-models-callbacks| section.

Intended use-case it to resubmit past prompts, and responses, so LLM can
continue a longer train of thought.  Example: >
  function! Context(configurations, state) abort
    let l:endpoint = split(a:configurations.api.url, '/')[-1]
    let l:endpoint = split(l:endpoint, '?')[0]

    let l:context_size = 5
    let l:messages = a:state.messages[max([len(a:state.messages) - l:context_size, 0]):]

    if !len(l:messages)
      if l:endpoint == 'chat'
        return []
      elseif l:endpoint == 'generate'
        return ''
      else
        throw 'Unknown endpoint ->' . a:configurations.api.url
      endif
    endif

    if l:endpoint == 'chat'
      return mapnew(l:messages, { _index, entry ->
            \   { 'role': entry.message.role, 'content': entry.message.content }
            \ })
    elseif l:endpoint == 'generate'
      return join(mapnew(l:messages, { _index, entry ->
            \   entry.message.role . ': ' . entry.message.content
            \ }), '\n\n')
    else
      throw 'Unknown endpoint ->' . a:configurations.api.url
    endif
  endfunction
<
                       *proompter-configuration-models-prompt-callbacks-input*
Default `context` callback is provided {string} as the first parameter, and
{dictionary} references to `g:proompter`, and `g:proompter_state`, as second
and third parameters.  And should return a {string} or dictionary {list},
depending on API endpoint, as described in
|proompter-configuration-models-callbacks| section.

Intended use-case is adding formatting, and/or additional information, based
on input.  Example: >
  ""
  " WARNING: this is a naive, and dangerous, method for obtaining doc-comments
  function! Input(value, _configurations, _state) abort
    let l:endpoint = split(a:configurations.api.url, '/')[-1]
    let l:endpoint = split(l:endpoint, '?')[0]

    if &filetype != 'python'
      if l:endpoint == 'chat'
        return [{ 'role': 'user', 'content': a:value }]
      elseif l:endpoint == 'generate'
        return a:value
      else
        throw 'Unknown endpoint ->' . a:configurations.api.url
      endif
    endif

    let l:docs_found = {}
    let l:pattern = '\v<\w+(\()@='
    for l:line in split(a:value, '\n')
      let [l:key, l:index_start, l:index_end] = matchstrpos(l:line, l:pattern, 0)
      while l:index_start > -1 && l:index_end > -1
        if !len(get(l:docs_found, l:key, ''))
          let l:python_help = [
                \   'pythonx << EOF',
                \   'try:',
                \   '  help(' . l:key . ')',
                \   'except NameError:',
                \   '  import ' . split(l:key, '\.')[0],
                \   '  help(' . l:key . ')',
                \   'EOF',
                \ ]

          let l:docs_entry = execute join(l:python_help_lines, "\n")

          let l:docs_found[l:key] = l:docs_entry
        endif
        let [l:key, l:index_start, l:index_end] = matchstrpos(l:line, l:pattern, l:index_end)
      endwhile
    endfor

    if l:endpoint == 'chat'
      if !len(l:docs_found)
        return [{ 'role': 'user', 'content': a:value }]
      endif

      return extend(
            \   [{ 'role': 'user', 'content': 'Here are all relevant Python documentation I can find' }]
            \   mapnew(l:docs_found, { _key, doc_value ->
            \     { 'role': 'user', 'content': doc_value }
            \   }),
            \   [{ 'role': 'user', 'content': a:value }]
            \ )
    elseif l:endpoint == 'generate'
      return a:value
    else
      throw 'Unknown endpoint ->' . a:configurations.api.url
    endif
  endfunction
<
                        *proompter-configuration-models-prompt-callbacks-post*
Default `context` callback is provided {dictionary} as the first parameter
with results of previous callbacks, and {dictionary} references to
`g:proompter`, and `g:proompter_state`, as second and third parameters.  And
should return a {string} or dictionary {list}, depending on API endpoint, as
described in |proompter-configuration-models-callbacks| section.

Intended use-case is merging results from previous callbacks, preforming any
sanity checks, prior to returning result for consumption by either;
  - |proompter#SendPrompt|
  - |proompter#SendHighlightedText|

Example: >
  function! Post(prompt_callbacks_data, configurations, _state) abort
    let l:endpoint = split(a:configurations.api.url, '/')[-1]
    let l:endpoint = split(l:endpoint, '?')[0]

    let l:keys = [ 'preamble', 'context', 'input' ]
    if l:endpoint == 'chat'
      let l:results = []
      for l:key in l:keys
        let l:messages = get(a:prompt_callbacks_data, l:key, [])
        if len(l:line)
          call extend(l:results, l:messages)
        endif
      endfor
      return l:results
    elseif l:endpoint == 'generate'
      let l:results = []
      for l:key in l:keys
        let l:line = get(a:prompt_callbacks_data, l:key, '')
        if len(l:line)
          call add(l:results, l:line)
        endif
      endfor
      return join(l:results, "\n")
    else
      throw 'Unknown endpoint ->' . a:configurations.api.url
    endif
  endfunction
<
Note: in the future the |proompter#SendPrompt| function, and those that call
it automatically detect state of `g:proompter.api.url` endpoint, and call
either |proompter#SendPromptToGenerate| or |proompter#SendPromptToChat| as
necessary.  This means you may choose to instead focus on a particular API
endpoint, instead of preforming checks at every callback X-D

==============================================================================
3. Functions                                             *proompter-functions*

There are many functions provided by this plugin most, if not all, organized
under name-spaces provided by `autoload/` directory and file paths.
Development details about auto loading maybe found via; |autoload-functions|,
|autoload| |E746|, and section |52.2|.

proompter#SendPrompt({string}, {dictionary}, {dictionary})  *proompter#SendPrompt*
                Detects API endpoint in `configurations.api.url` and forward
                call to one of the following functions;
                  - |proompter#SendPromptToChat|
                  - |proompter#SendPromptToGenerate|

                Parameters:
                  - {string} `value` What to send to LLM
                  - {dictionary} `configurations` Defaults to `g:proompter`
                  - {dictionary} `state` Defaults to `g:proompter_state`

proompter#SendHighlightedText({dictionary})    *proompter#SendHighlightedText*
                Send range or visually selected text to LLM as newline
                separated {string} by way of |proompter#SendPrompt|

                Parameters:
                  - {string} prefix_input Optional text prefixed to line range
                  - {dictionary} |proompter-configuration| passed un-mutated

proompter#SendPromptToChat({string}, {dictionary}, {dictionary}) *proompter#SendPromptToChat*
                Using options read from {dictionary} try to format {string} as
                value for encoding into JSON value sent to LLM.

                Saves the `value` to `g:proompter_state.messages` >
  {
    "model": "codellama",
    "created_at": "2024-09-20T23:25:06.675058548Z",
    "message": {
      "role": "user",
      "content": "Tell me Vim is the best text editor.",
      "image": v:null,
    }
  }
<
                Parameters:
                  - {string} `value` What to send to LLM
                  - {dictionary} `configurations` Defaults to `g:proompter`
                  - {dictionary} `state` Defaults to `g:proompter_state`

proompter#SendPromptToGenerate({string}, {dictionary}, {dictionary})  *proompter#SendPromptToGenerate*
                Using options read from {dictionary} try to format {string} as
                value for encoding into JSON value sent to LLM.

                Saves the `value` to `g:proompter_state.messages` >
  {
    "model": "codellama",
    "created_at": "2024-09-20T23:25:06.675058548Z",
    "message": {
      "role": "user",
      "content": "Tell me Vim is the best text editor.",
      "image": v:null,
    }
  }
<
                Parameters:
                  - {string} `value` What to send to LLM
                  - {dictionary} `configurations` Defaults to `g:proompter`
                  - {dictionary} `state` Defaults to `g:proompter_state`

                Note: if `g:proompter.models['model_name'].prompt_callbacks` is
                defined then resulting prompt sent to LLM is built by
                `prompt_callbacks.post` callback, if available, else the
                following are appended in order; `preamble`, `prompt`, `input`.

                Else if `g:proompter.models['model_name'].prompt_callbacks` is
                not defined `g:proompter.models['model_name'].data.prompt` is
                prepended to `value` before being sent to LLM at
                `g:proompter.api.url` via channel proxy.

                Note: `g:proompter.select.model_name` defines `'model_name'`
                key in above note, and elsewhere within |proompter.txt| file.

                Check following internal references for additional details;
                  - |proompter-configuration| -- {dictionary} read for reasons
                  - |proompter#format#HTTPPost| -- Builds HTTP request
                  - |proompter#channel#GetOrSetOpen| -- Prepares |channel-raw|

                Example `g:proompter.models['model_name'].prompt_callbacks`;
                  - |proompter#callback#prompt#generate#Preamble| Prefixes
                    prompt with text to prime LLM behavior.
                  - |proompter#callback#prompt#generate#Context| Prefixes
                    prompt with history of last few prompt/response-s.
                  - |proompter#callback#prompt#generate#Input| Provides
                    additional formatting to prompt input.
                  - |proompter#callback#prompt#generate#Post| Concatenates
                    results of previous callbacks, where defined, and writes
                    to configured Vim buffer.

proompter#callback#channel#CompleteToHistory({string}, {dictionary}, {dictionary}, ...)  *proompter#callback#channel#CompleteToHistorye*
                Handle fully resolved HTTP response from channel proxied API.

                By default is attached to `g:proompter_state.channel` when
                `g:proompter.models['model_name'].data.stream` is `v:false`,
                and no custom callback function is defined at
                `g:proompter.channel.options.callback`, check
                |proompter#callback#channel#StreamToHistory| for details when
                `g:proompter.models['model_name'].data.stream` is `v:true`.

                Saves HTTP response data returned from API in
                `g:proompter_state.messages` with _shape_ similar to; >
  {
    'model': 'codellama',
    'created_at': '2024-09-20T23:25:06.675058548Z',
    'done': true,
    'done_reason': 'stop',
    'message': {
      'role': 'assistant',
      'content': 'Vim is the best',
      'images': v:null,
    },
  }
<
                Parameters:
                  - {string} `api_response` HTTP response
                  - {dictionary} `configurations` Defaults to `g:proompter`
                  - {dictionary} `state` Defaults to `g:proompter_state`

                Expects HTTP response similar to: >
  HTTP/1.0 200 OK
  Server: SimpleHTTP/0.6 Python/3.12.6
  Date: Fri, 20 Sep 2024 23:25:06 GMT
  Content-Type: application/json

  {"model":"codellama","created_at":"2024-09-20T23:25:01.01645329Z","response":"V","done":false}
  {"model":"codellama","created_at":"2024-09-20T23:25:01.177902785Z","response":"im","done":false}
  {"model":"codellama","created_at":"2024-09-20T23:25:01.341776729Z","response":" is","done":false}
  {"model":"codellama","created_at":"2024-09-20T23:25:01.506237509Z","response":" the","done":false}
  {"model":"codellama","created_at":"2024-09-20T23:25:01.670272033Z","response":" best","done":false}
  ...
  {"model":"codellama","created_at":"2024-09-20T23:25:06.675058548Z","response":"","done":true,"done_reason":"stop","context":[...],"total_duration":7833808817,"load_duration":10021098,"prompt_eval_count":31,"prompt_eval_duration":2122796000,"eval_count":35,"eval_duration":5658536000}
<
proompter#callback#channel#StreamToHistory({string}, {string}, ...)  *proompter#callback#channel#StreamToHistory*
                Handle stream of HTTP responses from channel proxied API.

                By default is attached to `g:proompter_state.channel` when
                `g:proompter.models['model_name'].data.stream` is `v:true`,
                and no custom callback function is defined at
                `g:proompter.channel.options.callback`, check
                |proompter#callback#channel#CompleteToHistory| for details
                when `g:proompter.models['model_name'].data.stream` is
                `v:false`.

                Saves HTTP response data returned from API in
                `g:proompter_state.messages` with _shape_ similar to; >
  {
    'model': 'codellama',
    'created_at': '2024-09-20T23:25:06.675058548Z',
    'done': true,
    'done_reason': 'stop',
    'message': {
      'role': 'assistant',
      'content': 'Vim is the best',
      'images': v:null,
    },
  }
<
                Parameters:
                  - {string} `api_response` HTTP response
                  - {dictionary} `configurations` Defaults to `g:proompter`
                  - {dictionary} `state` Defaults to `g:proompter_state`

                Expects HTTP response similar to: >
  HTTP/1.0 200 OK
  Server: SimpleHTTP/0.6 Python/3.12.6
  Date: Fri, 20 Sep 2024 23:25:06 GMT
  Content-Type: application/json

  {"model":"codellama","created_at":"2024-09-20T23:25:01.01645329Z","response":"V","done":false}

proompter#callback#channel#StreamToBuffer({dictionary})  *proompter#callback#channel#StreamToBuffer*
                Handle stream of HTTP responses from channel proxied API by
                appending to buffer history, either appending to value the
                last entry of `{"type": "response"}` or by creating a new
                entry, as well as outputting to target text buffer.

                Parameters:
                  - {string} `api_response` HTTP response
                  - {dictionary} `configurations` Defaults to `g:proompter`
                  - {dictionary} `state` Defaults to `g:proompter_state`
                  - {string} `out_bufnr` Default to "proompt-log.md"
                    WARNING: no checks are currently implemented by default to
                    prevent writing to preexisting buffers, or special buffers
                    such as a terminal session!

                Example: expects series of `api_response` similar to 1 of 2 >
  HTTP/1.0 200 OK
  Server: SimpleHTTP/0.6 Python/3.12.6
  Date: Fri, 20 Sep 2024 23:25:06 GMT
  Content-Type: application/json

  {"model":"codellama","created_at":"2024-09-20T23:25:01.01645329Z","response":"V","done":false}
<
                Example: expects series of `api_response` similar to 2 of 2 >
  HTTP/1.0 200 OK
  Server: SimpleHTTP/0.6 Python/3.12.6
  Date: Fri, 20 Sep 2024 23:25:01 GMT
  Content-Type: application/json

  {"model":"codellama","created_at":"2024-09-20T23:25:01.177902785Z","response":"im","done":false}
<
                  WARNING: expects buffer history to be {dictionary} {list}
                  _shaped_ similar to >
  [
    {
      'message': {
        'role': 'user',
        'content': 'Tell me in one sentence why Vim is the best text editor.',
      },
    },
    {
      'model': 'codellama',
      'created_at': '2024-09-20T23:25:06.675058548Z',
      'done': true,
      'done_reason': 'stop',
      'message': {
        'role': 'assistant',
        'content': 'Vim',
        'images': v:null,
      },
    },
  ]
<

                Example: configuration snippet >
  let g:proompter = {
        \   'channel': {
        \     'options': {
        \       'callback': { api_response, configurations, state ->
        \         proompter#callback#channel#StreamToBuffer(
        \           api_response,
        \           configurations,
        \           state,
        \           v:null,
        \         )
        \       },
        \     },
        \   },
        \ }
<
proompter#callback#prompt#generate#Preamble({dictionary})  *proompter#callback#prompt#generate#Preamble*
                When there is no message history, or history would cause
                `prompt_callbacks.post` to drop this prefix, return start of
                prompt with content similar to following; >
  You an expert with javascript and delight in solving problems succinctly!

  Content between "<HISTORY>" and "</HISTORY>"  may provide
  additional context to the following input.

  Input will be surrounded by "<PROOMPT>" and "</PROOMPT>" tags,
  please pay most attention to the last instance.
<
                Parameter: {dictionary} `kwargs` Has following defined
                  - {dictionary} `configurations` Defaults to `g:proompter`
                  - {number} `context_size` Max prompt/response results that
                    are re-shared
                  - {string} `filetype` What file type is operated on
                  - {dictionary} `history_tags` With `start` and `stop` values
                    defined to help clue-in LLM of past context
                  - {dictionary} `input_tags` With `start` and `stop` values
                    defined to help LLM focus on latest input

                Example: configuration snippet >
  let g:proompter = {
        \   'select': {
        \     'model_name': 'codellama',
        \   },
        \   'models': {
        \     'codellama': {
        \       'prompt_callbacks': {
        \         'preamble': { configurations, _state ->
        \           proompter#callback#prompt#generate#Preamble({
        \             'configurations': configurations,
        \             'filetype': 'javascript',
        \             'history_tags': { 'start': '<HISTORY>', 'end': '</HISTORY>'},
        \             'input_tags': { 'start': '<PROOMPT>', 'end': '</PROOMPT>'},
        \             'response_tags': { 'start': '<RESPONSE>', 'end': '</RESPONSE>'},
        \           })
        \         },
        \       },
        \     },
        \   },
        \ }
<
proompter#callback#prompt#generate#Input({dictionary})  *proompter#callback#prompt#generate#Input*
                Returns string formatted from `kwargs.input` and
                `kwargs.input_tags` similar to >
  <PROOMPT>
  Tell me in one sentence why Vim is the best editor for programming.
  </PROOMPT>
<
                Parameter: {dictionary} `kwargs` Has following defined
                  - {string} `value` Text to prompt LLM with
                  - {dictionary} `input_tags` With `start` and `stop` values
                    defined to help LLM focus on latest input
                  - {dictionary} `configurations` Defaults to `g:proompter`

                Example: configuration snippet >
  let g:proompter = {
        \   'select': {
        \     'model_name': 'codellama',
        \   },
        \   'models': {
        \     'codellama': {
        \       'prompt_callbacks': {
        \         'input': { value, configurations, _state ->
        \           proompter#callback#prompt#generate#Input({
        \             'value': value,
        \             'configurations': configurations,
        \             'input_tags': { 'start': '<PROOMPT>', 'end': '</PROOMPT>'},
        \           })
        \         },
        \       },
        \     },
        \   },
        \ }
<
                Dev tip: remove tags surrounding later via something like >
  echo substitute(_input_, '</\?PROOMPT>', '', 'g')
<
proompter#callback#prompt#generate#Post({dictionary})  *proompter#callback#prompt#generate#Post*
                Merge together outputs from other prompt callback functions as
                well as history of past input/response-s into a single string
                to pass to LLM.

                Parameter: {dictionary} `kwargs` - Has the following defined
                  - {dictionary} `data` with `preamble`, `prompt`, and `input` keys
                    pointing to string values
                  - {number} `context_size` Max results from
                    `g:proompter_state.messages` to provide LLM context
                  - {number} `out_bufnr` buffer number used for output, if
                    `v:null` one will be created automatically via
                    `proompter#lib#GetOrMakeProomptBuffer` with the name
                    "proompt-log.md", or you man set a {string} value to
                    customize the buffer name.
                  - {dictionary} `history_tags` With `start` and `stop` values
                    defined to help clue-in LLM of past context
                  - {dictionary} `configurations` Defaults to `g:proompter`

                  WARNING: expects `g:proompter_state.messages` to be
                  dictionary list _shaped_ similar to; >
  [
    {
      'message': {
        'role': 'user',
        'content': 'Tell me in one sentence why Vim is the best text editor.',
      },
    },
    {
      'model': 'codellama',
      'created_at': '2024-09-20T23:25:06.675058548Z',
      'done': true,
      'done_reason': 'stop',
      'message': {
        'role': 'assistant',
        'content': 'Vim',
        'images': v:null,
      },
    },
  ]
<
                Example: configuration snippet >
  let g:proompter = {
        \   'select': {
        \     'model_name': 'codellama',
        \   },
        \   'models': {
        \     'codellama': {
        \       'prompt_callbacks': {
        \         'post': { prompt_callbacks_data, configurations, _state ->
        \           proompter#callback#prompt#generate#Post({
        \             'data': prompt_callbacks_data,
        \             'configurations': configurations,
        \             'out_bufnr': v:null,
        \           })
        \         },
        \       },
        \     },
        \   },
        \ }
<
proompter#channel#CreateOptions({dictionary}, {dictionary})  *proompter#channel#CreateOptions*
                Creates and, if necessary, defaults options for a Vim channel
                object based on the configurations provided.  If configuration
                does not contain a 'callback' option, it will add one based on
                the type of data stream.

                Parameters:
                  - {dictionary} `configurations` (|proompter-configuration|)
                    that contains properties to pass to |ch_open()|
                  - {dictionary} `state` (|proompter-state|) that contains
                    state shared between calls

                Returns: {dictionary} containing options for Vim channel.

proompter#channel#GetOrSetOpen({dictionary}, {dictionary})  *proompter#channel#GetOrSetOpen*
                Attempts to open, or re-open, a channel from given
                configurations.  Warns when previously stored channel has a
                "fail" state, and throws error when previously stored channel
                has a "buffered" state.

                Parameters:
                  - {dictionary} `configurations` (|proompter-configuration|)
                    that contains properties to pass to |ch_open()|
                  - {dictionary} `state` (|proompter-state|) that contains
                    state shared between calls

                Returns: Vim {channel} - A reference to an open Vim channel.

proompter#format#HTTPPost({string}, {dictionary})  *proompter#format#HTTPPost*
                Formats an HTTP POST request from data and configurations,
                returning a formatted string for further use.

                Parameters:
                  - {string} `data` Body payload to be POST-ed.  The function
                    will convert this into a JSON string if necessary.
                  - {dictionary} `configurations` (|proompter-configuration|)
                    that contains properties to build POST request for proxy.

                Returns: {string} Formatted HTTP POST request as a string.

proompter#lib#GetOrMakeProomptBuffer({string})  *proompter#lib#GetOrMakeProomptBuffer*
                Return bufnr of new buffer after setting it up for logging proompts

                Parameter:
                  - {string} or {v:null} `buffer_name` Name output buffer
                    should use, or pass `v:null` to have default `proompt-log.md`

                Note: this function is used internally by;
                  - |proompter#callback#channel#StreamToBuffer|
                  - |proompter#callback#prompt#generate#Post|
                ...  Nether of which are used by default.

proompter#parse#HeadersFromHTTPResponse({string})  *proompter#parse#HeadersFromHTTPResponse*
                Convert HTTP headers into Vim dictionary, or returns empty
                dictionary if no headers are parsed from input data.

                Example: HTTP response >
  Server: SimpleHTTP/0.6 Python/3.12.6
  Date: Sat, 28 Sep 2024 23:29:00 GMT
  Content-Type: application/json

  {"model": "codellama", "created_at": "2024-09-28T23:29:00.299380014Z", "response": " V", "done": false}
<
                Example: resulting Vim dictionary >
  {
    'Content-Type': 'application/json',
    'Date': 'Sat, 28 Sep 2024 23:29:00 GMT',
    'Server': 'SimpleHTTP/0.6 Python/3.12.6'
  }
<
proompter#parse#JSONLinesFromHTTPResponse({string})  *proompter#parse#JSONLinesFromHTTPResponse*
                Returns {dictionary} {list} built by parsing body as JSON
                dictionaries, separated by any number of space,
                carriage-return, and/or newline characters.

                Example: HTTP response >
  Server: SimpleHTTP/0.6 Python/3.12.6
  Date: Sat, 28 Sep 2024 23:29:00 GMT
  Content-Type: application/json

  {"model": "codellama", "created_at": "2024-09-28T23:29:00.299380014Z", "response": " {", "done": false}
  {"model": "codellama", "created_at": "2024-09-20T23:25:01.670272033Z", "response": " \"foo", "done": true}
<
                Example: resulting Vim dictionary list >
  [
    {
      "model": "codellama",
      "created_at": "2024-09-28T23:29:00.299380014Z",
      "response": " {",
      "done": v:false
    },
    {
      "model": "codellama",
      "created_at": "2024-09-20T23:25:01.670272033Z",
      "response": ' "foo',
      "done": v:true
    }
  ]
<
proompter#parse#HTTPResponse({string})          *proompter#parse#HTTPResponse*
                Parses HTTP response data into `headers` {dictionary} and
                `body` {dictionary} list.  The 'headers' are anything before
                two consecutive carriage-return newline characters
                (`\r\n\r\n`), or any number of characters before the first
                opening curly brace (`{`) of the response, and the body
                is everything after that.

                Check the following functions for type details and examples;
                  - |proompter#parse#HeadersFromHTTPResponse|
                  - |proompter#parse#JSONLinesFromHTTPResponse|

                Returns: {dictionary} with keys `headers` and `body` defined.

                Note: callers _should_ check the length before assuming any
                sub-values to be present on top-level keys.

==============================================================================
4. Notes                                                     *proompter-notes*

During development and testing you may wish to modify configurations while
preserving defaults, here's an Ex mode one-liner to help with that; >
  source dev/tests.vim | source plugin/proompter.vim
<
... Additionally it may be wise to also clear some of the shared state; >
  let g:proompter_state.messages = []
<

==============================================================================
5. Common Errors                                            *proompter-errors*

                When attempting to prompt;  >
  E906: Not an open channel
<
                It may be that your SystemD killed the proxy, or Ollama,
                service(s) try the following in a terminal; >
        sudo systemctl restart ollama.service

        python scripts/proompter-channel-proxy.py --verbose
<

 vim:tw=78:ts=8:ft=help:norl:expandtab
