*proompter.txt*      For Vim version 9.1       Last change: 2024 Sep 22


                            Proompter    by S0AndS0


Proompter                                              *proompter*

1. Functions                                       |proompter-functions|
2. Configuration                               |proompter-configuration|
3. Notes                                               |proompter-notes|
4. Common Errors                                      |proompter-errors|

==============================================================================
1. Functions                                       *proompter-functions*

There are many functions provided by this plugin most, if not all, organized
under name-spaces provided by `autoload/` directory and file paths.
Development details about auto loading maybe found via; |autoload-functions|,
|autoload| |E746|, and section |52.2|.

proompter#SendPrompt({string}, {dictionary})          *proompter#SendPrompt()*
                Using options read from {dictionary} try to format {string} as
                value for encoding into JSON value sent to LLM.  For many
                use-cases this function is your entry point for this plugin.

                Saves the `value` to `g:proompter_state.history` >
  {
    "type": "prompt",
    "value": "Tell me in one sentence why Vim is the best for programming."
  }
<

                Parameters:
                  - {string} `value` What to send to LLM
                  - {dictionary} `configurations` Defaults to `g:proompter`

                Note: if `g:proompter.models['model_name'].prompt_callbacks` is
                defined then resulting prompt sent to LLM is built by
                `prompt_callbacks.post` callback, if available, else the
                following are appended in order; `pre`, `prompt`, `input`.

                Else if `g:proompter.models['model_name'].prompt_callbacks` is
                not defined `g:proompter.models['model_name'].data.prompt` is
                prepended to `value` before being sent to LLM at
                `g:proompter.api.url` via channel proxy.

                Note: `g:proompter.select.model_name` defines `'model_name'`
                key in above note, and elsewhere within |proompter.txt| file.

                Check following internal references for additional details;
                  - |proompter-configuration| -- {dictionary} read for reasons
                  - |proompter#format#HTTPPost| -- Builds HTTP request
                  - |proompter#channel#GetOrSetOpen| -- Prepares |channel-raw|

                Example `g:proompter.models['model_name'].prompt_callbacks`;
                  - |<proompter#callback#prompt#Pre| -- May prefix prompt

proompter#SendHighlightedText({dictionary})    *proompter#SendHighlightedText*
                Send range or visually selected text to LLM as newline
                separated {string} by way of |proompter#SendPrompt|

                Parameters:
                  - {string} prefix_input Optional text prefixed to line range
                  - {dictionary} |proompter-configuration| passed un-mutated

proompter#callback#channel#CompleteToHistory({string}, {string}, ...)  *proompter#callback#channel#CompleteToHistorye*
                Handle fully resolved HTTP response from channel proxied API.

                By default is attached to `g:proompter_state.channel` when
                `g:proompter.models['model_name'].data.stream` is `v:false`,
                and no custom callback function is defined at
                `g:proompter.channel.options.callback`, check
                |proompter#callback#channel#StreamToHistory| for details when
                `g:proompter.models['model_name'].data.stream` is `v:true`.

                Saves response returned from API in `g:proompter_state.histor`
                with _shape_ similar to; >
  {
    "type": "response",
    "value": "Vim is the best"
  }
<
                Parameters:
                  - {string} `channel_response` Status reported by Vim, eg.
                    "channel 529 closed".
                  - {string} `api_response` HTTP response similar to: >
  HTTP/1.0 200 OK
  Server: SimpleHTTP/0.6 Python/3.12.6
  Date: Fri, 20 Sep 2024 23:25:06 GMT
  Content-Type: application/json


  {"model":"codellama","created_at":"2024-09-20T23:25:01.01645329Z","response":"V","done":false}
  {"model":"codellama","created_at":"2024-09-20T23:25:01.177902785Z","response":"im","done":false}
  {"model":"codellama","created_at":"2024-09-20T23:25:01.341776729Z","response":" is","done":false}
  {"model":"codellama","created_at":"2024-09-20T23:25:01.506237509Z","response":" the","done":false}
  {"model":"codellama","created_at":"2024-09-20T23:25:01.670272033Z","response":" best","done":false}
  ...
  {"model":"codellama","created_at":"2024-09-20T23:25:06.675058548Z","response":"","done":true,"done_reason":"stop","context":[...],"total_duration":7833808817,"load_duration":10021098,"prompt_eval_count":31,"prompt_eval_duration":2122796000,"eval_count":35,"eval_duration":5658536000}
<
proompter#callback#channel#StreamToHistory({string}, {string}, ...)  *proompter#callback#channel#StreamToHistory*
                Handle stream of HTTP responses from channel proxied API.

                By default is attached to `g:proompter_state.channel` when
                `g:proompter.models['model_name'].data.stream` is `v:true`,
                and no custom callback function is defined at
                `g:proompter.channel.options.callback`, check
                |proompter#callback#channel#CompleteToHistory| for details
                when `g:proompter.models['model_name'].data.stream` is
                `v:false`.

                Saves response streamed from API in `g:proompter_state.histor`
                with _shape_ similar to; >
  {
    "type": "response",
    "value": "V"
  }
<
                Parameters:
                  - {string} `channel_response` Status reported by Vim, eg.
                    "channel 529 closed".
                  - {string} `api_response` HTTP response similar to: >
  HTTP/1.0 200 OK
  Server: SimpleHTTP/0.6 Python/3.12.6
  Date: Fri, 20 Sep 2024 23:25:06 GMT
  Content-Type: application/json


  {"model":"codellama","created_at":"2024-09-20T23:25:01.01645329Z","response":"V","done":false}

proompter#callback#channel#StreamToBuffer({dictionary})  *proompter#callback#channel#StreamToBuffer*
                Handle stream of HTTP responses from channel proxied API by
                appending to buffer history, either appending to value the
                last entry of `{"type": "response"}` or by creating a new
                entry, as well as outputting to target text buffer.

                Parameter: {dictionary} `kwargs` Has following defined
                  - {string} `channel_response` Status reported by Vim, eg.
                    'channel 529 closed'
                  - {string} `response_tag` XML tag name to surround response
                    with
                  - {dictionary} `response_tag` With `start` and `stop` values
                    defined to help LLM focus on latest input
                  - {dictionary} `out_bufnr` 'state' and `out` keys pointing
                    to buffer number callback should use for state and output.
                  - {string} `api_response` HTTP response _shape_ similar to
                    following examples

              Example: expects series of `api_response` similar to 1 of 2 >
  HTTP/1.0 200 OK
  Server: SimpleHTTP/0.6 Python/3.12.6
  Date: Fri, 20 Sep 2024 23:25:06 GMT
  Content-Type: application/json

  {"model":"codellama","created_at":"2024-09-20T23:25:01.01645329Z","response":"V","done":false}
<
              Example: expects series of `api_response` similar to 2 of 2 >
  HTTP/1.0 200 OK
  Server: SimpleHTTP/0.6 Python/3.12.6
  Date: Fri, 20 Sep 2024 23:25:01 GMT
  Content-Type: application/json

  {"model":"codellama","created_at":"2024-09-20T23:25:01.177902785Z","response":"im","done":false}
<

                Warning: expects buffer history to be dictionary list _shaped_
                similar to >
  [
    {
      "type": "prompt",
      "value": "... Maybe a question about a technical topic...",
    },
    {
      "type": "response",
      "value": "Are your finger-tips talking to you too?",
    }
  ]
<

                Example: configuration snippet >
  let g:proompter = {
        \   'channel': {
        \     'options': {
        \       'callback': { channel_response, api_response ->
        \         proompter#callback#channel#StreamToBuffer({
        \           'channel_response': channel_response,
        \           'api_response': api_response,
        \           'response_tag': 'RESPONSE',
        \           'buffers': { 'state': bufnr('%'), 'out': v:null },
        \         })
        \       },
        \     },
        \   },
        \ }
<

<proompter#callback#prompt#Pre({dictionary})   *proompter#callback#prompt#Pre*

                When there is no history, or history would cause
                `prompt_callbacks.post` to drop this prefix, return start of
                prompt with content similar to following; >
  You an expert with javascript and delight in solving problems succinctly!

  Content between "<HISTORY>" and "</HISTORY>"  may provide
  additional context to the following input.

  Input will be surrounded by "<PROOMPT>" and "</PROOMPT>" tags,
  please pay most attention to the last instance.
<
                Parameter: {dictionary} `kwargs` Has following defined
                  - {number} `context_size` Max prompt/response results that
                    are re-shared
                  - {string} `filetype` What file type is operated on
                  - {dictionary} `history_tags` With `start` and `stop` values
                    defined to help clue-in LLM of past context
                  - {dictionary} `input_tags` With `start` and `stop` values
                    defined to help LLM focus on latest input

                Example: configuration snippet >
    let g:proompter = {
          \   'select': {
          \     'model_name': 'codellama',
          \   },
          \   'models': {
          \     'codellama': {
          \       'prompt_callbacks': {
          \         'pre': { ->
          \           <proompter#callback#prompt#Pre({
          \             'context_size': 5,
          \             'filetype': &filetype,
          \             'history_tags': { 'start': '<HISTORY>', 'end': '</HISTORY>'},
          \             'input_tags': { 'start': '<PROOMPT>', 'end': '</PROOMPT>'},
          \           })
          \         },
          \       },
          \     },
          \   },
          \ }
<
proompter#callback#prompt#Input({dictionary}) *proompter#callback#prompt#Input*
                Returns string formatted from `kwargs.input` and
                `kwargs.input_tags` >

  <PROOMPT>
  Tell me in one sentence why Vim is the best editor for programming.
  </PROOMPT>
<
                Parameter: {dictionary} `kwargs` Has following defined
                  - {string} `value` Text to prompt LLM with
                  - {dictionary} `input_tags` With `start` and `stop` values
                    defined to help LLM focus on latest input

                Example: configuration snippet >
  let g:proompter = {
        \   'select': {
        \     'model_name': 'codellama',
        \   },
        \   'models': {
        \     'codellama': {
        \       'prompt_callbacks': {
        \         'input': { value ->
        \           proompter#callback#prompt#Input({
        \             'value': value,
        \             'input_tag': 'PROOMPT',
        \           })
        \         },
        \       },
        \     },
        \   },
        \ }
<
                Dev tip: remove tags surrounding later via something like >
  echo substitute(_input_, '</\?PROOMPT>', '', 'g')
<
proompter#callback#prompt#Post({dictionary})  *proompter#callback#prompt#Post*
                Merge together outputs from other prompt callback functions as
                well as history of past input/response-s into a single string
                to pass to LLM.

                Parameter: {dictionary} `kwargs` - Has the following defined
                  - {dictionary} `data` with `pre`, `prompt`, and `input` keys
                    pointing to string values
                  - {number} `context_size` Max results from
                    `g:proompter_state.history` to provide LLM context
                  - {number} `out_bufnr` buffer number used for output, if
                    `v:null` one will be created automatically via
                    `proompter#lib#GetOrMakeProomptBuffer` with the name
                    "proompt-log.md", or you man set a {string} value to
                    customize the buffer name.
                  - {dictionary} `history_tags` With `start` and `stop` values
                    defined to help clue-in LLM of past context

                  Warning: expects `g:proompter_state.history` to be
                  dictionary list _shaped_ similar to; >
  {
    "type": "prompt",
    "value": "some text"
  }
<
                Example: configuration snippet >
  let g:proompter = {
        \   'select': {
        \     'model_name': 'codellama',
        \   },
        \   'models': {
        \     'codellama': {
        \       'prompt_callbacks': {
        \         'post': { prompt_callbacks_data ->
        \           proompter#callback#prompt#Post({
        \             'data': prompt_callbacks_data,
        \             'context_size': 5,
        \             'history_tags': { 'start': '<HISTORY>', 'end': '</HISTORY>'},
        \             'out_bufnr': v:null,
        \           })
        \         },
        \       },
        \     },
        \   },
        \ }
<
proompter#channel#CreateOptions({dictionary}) *proompter#channel#CreateOptions*
                Creates and, if necessary, defaults options for a Vim channel
                object based on the configurations provided.  If configuration
                does not contain a 'callback' option, it will add one based on
                the type of data stream.

                Parameters:
                  - {dictionary} `configurations` (|proompter-configuration|)
                    that contains properties to pass to |ch_open()|

                Returns: {dictionary} containing options for Vim channel.

proompter#channel#GetOrSetOpen({dictionary})  *proompter#channel#GetOrSetOpen*
                Attempts to open, or re-open, a channel from given
                configurations.  Warns when previously stored channel has a
                "fail" state, and throws error when previously stored channel
                has a "buffered" state.

                Parameters:
                  - {dictionary} `configurations` (|proompter-configuration|)
                    that contains properties to pass to |ch_open()|

                Returns: Vim {channel} - A reference to an open Vim channel.

proompter#format#HTTPPost({string}, {dictionary})  *proompter#format#HTTPPost*
                Formats an HTTP POST request from data and configurations,
                returning a formatted string for further use.

                Parameters:
                  - {string} `data` Body payload to be POST-ed.  The function
                    will convert this into a JSON string if necessary.
                  - {dictionary} `configurations` (|proompter-configuration|)
                    that contains properties to build POST request for proxy.

                Returns: {string} Formatted HTTP POST request as a string.

proompter#lib#GetOrMakeProomptBuffer({string})  *proompter#lib#GetOrMakeProomptBuffer*
                Return bufnr of new buffer after setting it up for logging proompts

                Parameter:
                  - {string} or {v:null} `buffer_name` Name output buffer
                    should use, or pass `v:null` to have default `proompt-log.md`

                Note: this function is used internally by;
                  - |proompter#callback#channel#StreamToBuffer|
                  - |proompter#callback#prompt#Post|
                ...  Nether of which are used by default.

proompter#parse#HeadersFromHTTPResponse({string})  *proompter#parse#HeadersFromHTTPResponse*
                Convert HTTP headers into Vim dictionary, or returns empty
                dictionary if no headers are parsed from input data.

                Example: HTTP response >
  Server: SimpleHTTP/0.6 Python/3.12.6
  Date: Sat, 28 Sep 2024 23:29:00 GMT
  Content-Type: application/json

  {"model": "codellama", "created_at": "2024-09-28T23:29:00.299380014Z", "response": " V", "done": false}
<
                Example: resulting Vim dictionary >
  {
    'Content-Type': 'application/json',
    'Date': 'Sat, 28 Sep 2024 23:29:00 GMT',
    'Server': 'SimpleHTTP/0.6 Python/3.12.6'
  }
<
proompter#parse#JSONLinesFromHTTPResponse({string})  *proompter#parse#JSONLinesFromHTTPResponse*
                Returns {dictionary} {list} built by parsing body as JSON
                dictionaries, separated by any number of space,
                carriage-return, and/or newline characters.

                Example: HTTP response >
  Server: SimpleHTTP/0.6 Python/3.12.6
  Date: Sat, 28 Sep 2024 23:29:00 GMT
  Content-Type: application/json

  {"model": "codellama", "created_at": "2024-09-28T23:29:00.299380014Z", "response": " {", "done": false}
  {"model": "codellama", "created_at": "2024-09-20T23:25:01.670272033Z", "response": " \"foo", "done": true}
<
                Example: resulting Vim dictionary >
  [
    {
      "model": "codellama",
      "created_at": "2024-09-28T23:29:00.299380014Z",
      "response": " {",
      "done": v:false
    },
    {
      "model": "codellama",
      "created_at": "2024-09-20T23:25:01.670272033Z",
      "response": ' "foo',
      "done": v:true}
  ]
<
proompter#parse#HTTPResponse({string})          *proompter#parse#HTTPResponse*
                Parses HTTP response data into `headers` {dictionary} and
                `body` {dictionary} list.  The 'headers' are anything before
                two consecutive carriage-return newline characters
                (`\r\n\r\n`), or any number of characters before the first
                opening curly brace (`{`) of the response, and the body
                is everything after that.

                Check the following functions for type details and examples;
                  - |proompter#parse#HeadersFromHTTPResponse|
                  - |proompter#parse#JSONLinesFromHTTPResponse|

                Returns: {dictionary} with keys `headers` and `body` defined.

                Note: callers _should_ check the length before assuming any
                sub-values to be present on top-level keys.

==============================================================================
2. Configuration                               *proompter-configuration*

                Default configurations currently are; >
  let g:proompter = {
        \   'select': {
        \     'model_name': 'codellama',
        \   },
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/generate',
        \   },
        \   'channel': {
        \     'address': '127.0.0.1:11435',
        \     'options': {
        \       'mode': 'raw',
        \       'callback': v:null,
        \     },
        \   },
        \   'models': {
        \     'codellama': {
        \       'prompt_callbacks': {
        \         'pre': v:null,
        \         'prompt': v:null,
        \         'input': v:null,
        \         'post': v:null,
        \       },
        \       'data': {
        \         'prompt': '',
        \         'raw': v:false,
        \         'stream': v:true,
        \       },
        \     },
        \   },
        \ }
<
                ...  Please define your customizations to load before this
                script to have defaults defaulted and customization correctly
                overwrite configurations.

                Example of defining custom callback functions; >
  let g:proompter = {
        \   'channel': {
        \     'options': {
        \       'callback': { channel_response, api_response ->
        \         proompter#callback#channel#StreamToBuffer({
        \           'channel_response': channel_response,
        \           'api_response': api_response,
        \           'response_tag': 'RESPONSE',
        \           'out_bufnr': v:null,
        \         })
        \       },
        \     },
        \   },
        \   'models': {
        \     'codellama': {
        \       'prompt_callbacks': {
        \         'pre': { ->
        \           proompter#callback#prompt#Pre({
        \             'context_size': 5,
        \             'filetype': 'javascript',
        \             'history_tags': { 'start': '<HISTORY>', 'end': '</HISTORY>'},
        \             'input_tags': { 'start': '<PROOMPT>', 'end': '</PROOMPT>'},
        \           })
        \         },
        \         'input': { value ->
        \           proompter#callback#prompt#Input({
        \             'value': value,
        \             'input_tag': 'PROOMPT',
        \           })
        \         },
        \         'post': { prompt_callbacks_data ->
        \           proompter#callback#prompt#Post({
        \             'data': prompt_callbacks_data,
        \             'context_size': 5,
        \             'history_tags': { 'start': '<HISTORY>', 'end': '</HISTORY>'},
        \             'out_bufnr': v:null,
        \           })
        \         },
        \       },
        \     },
        \   },
        \ }
<
==============================================================================
3. Notes                                               *proompter-notes*

During development and testing you may wish to modify configurations while
preserving defaults, here's an Ex mode one-liner to help with that; >
  source dev/tests.vim | source plugin/proompter.vim
<
... Additionally it may be wise to also clear some of the shared state; >
  let g:proompter_state.history = []
<

==============================================================================
4. Common Errors                                      *proompter-errors*

                When attempting to prompt;  >
  E906: Not an open channel
<
                It may be that your SystemD killed the proxy, or Ollama,
                service(s) try the following in a terminal; >
        sudo systemctl restart ollama.service

        python scripts/proompter-channel-proxy.py --verbose
<

 vim:tw=78:ts=8:ft=help:norl:expandtab
