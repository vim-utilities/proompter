*proompter.txt*	Provide integration with local Ollama LLM API
S0AndS0 <https://github.com/S0AndS0>                               *proompter*

==============================================================================
CONTENTS                                                  *proompter-contents*
  1. Configuration..........................................|proompter-config|
  2. Dictionaries............................................|proompter-dicts|
  3. Functions...........................................|proompter-functions|
  4. License...............................................|proompter-license|

==============================================================================
CONFIGURATION                                               *proompter-config*

                                                                 *g:proompter*
ProompterConfigurations that may be overwritten


{ProompterConfigurationsSelect} `select`
{ProompterConfigurationsAPI} `api`
{ProompterConfigurationsChannel} `channel`
{ProompterConfigurationsModels} `models`

Default: `g:proompter`~
>
  let s:defaults = {
        \   'select': {
        \     'model_name': 'codellama',
        \   },
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/generate',
        \     'prompt_callbacks': {
        \       'chat': {},
        \       'generate': {},
        \     },
        \   },
        \   'channel': {
        \     'address': '127.0.0.1:11435',
        \     'options': {
        \       'mode': 'raw',
        \       'callback': v:null,
        \     },
        \   },
        \   'models': {
        \     'codellama': {
        \       'prompt_callbacks': {
        \         'chat': {},
        \         'generate': {},
        \       },
        \       'data': {
        \         'raw': v:false,
        \         'stream': v:true,
        \       },
        \     },
        \   },
        \ }
<

                                                           *g:proompter_state*

Example: `g:proompter_state`~
>
  {
    "channel": v:null,
    "messages": [
      {
        "role": "system",
        "content": "You an expert with javascript",
      },
      {
        "role": "user",
        "content": "Tell me in one sentence Vim is the best text editor.",
      },
      {
        "role": "assistant",
        "content": "Vim is the best!",
      },
    ],
  }
<

This dictionary stores current `channel` used to communicate with API proxy,
and list of `messages` data sent to/from LLM(s)

{channel} `channel` Last used or current channel that is in use, or `v:null`
if no channel has been assigned within current running Vim session.

See: following built-in functions for more details about channels
|ch_open()|
|ch_info()|

{list} `messages` {dictionary} list of messages sent to, and received from,
Ollama API

See: official Ollama API documentation
https://github.com/ollama/ollama/blob/main/docs/api.md

==============================================================================
DICTIONARIES                                                 *proompter-dicts*

                                                   *proompter.APIResponseChat*

Example: /api/chat~
>
  {
    "model": "llama3.2",
    "created_at": "2023-12-12T14:13:43.416799Z",
    "message": {
      "role": "assistant",
      "content": "Hello! How are you today?"
    },
    "done": true,
    "total_duration": 5191566416,
    "load_duration": 2154458,
    "prompt_eval_count": 26,
    "prompt_eval_duration": 383809000,
    "eval_count": 298,
    "eval_duration": 4799921000
  }
<

Attribution:
https://github.com/ollama/ollama/blob/main/docs/api.md#chat-request-with-history

                                               *proompter.APIResponseGenerate*

Example: /api/generate~
>
  {
    "model": "llama3.2",
    "created_at": "2023-08-04T19:22:45.499127Z",
    "response": "The sky is blue because it is the color of the sky.",
    "done": true,
    "context": [1, 2, 3],
    "total_duration": 4935886791,
    "load_duration": 534986708,
    "prompt_eval_count": 26,
    "prompt_eval_duration": 107345000,
    "eval_count": 237,
    "eval_duration": 4289432000
  }
<

                                             *proompter.APIResponseNormalized*

Merged data from |APIResponseChat| and |APIResponseGenerate|

|dictionary| with shape similar to
>
  {
    "model": "llama3.2",
    "created_at": "2023-08-04T08:52:19.385406455-07:00",
    "message": {
      "role": "assistant",
      "content": "The",
      "images": v:null
      "tool_calls": v:null
    },
    "context": v:null,
    "done": v:false,
    "done_reason": v:null,
  }
<

                                                       *proompter.HTTPHeaders*

{string} `key` Whatever string was read between first colon (`:`) and start of
each line

{string} `value` Everything after first colon (`:`) and end of each line

Example: HTTP response data~
>
  HTTP/1.1 200 OK
  Server: SimpleHTTP/0.6 Python/3.12.6
  Date: Sat, 28 Sep 2024 23:29:00 GMT
  Content-Type: application/json

  {"key":"value"}
<

Example: HTTP Headers Vim dictionary~
>
  {
    'Content-Type': 'application/json',
    'Date': 'Sat, 28 Sep 2024 23:29:00 GMT',
    'Server': 'SimpleHTTP/0.6 Python/3.12.6',
  }
<

                                                        *proompter.HTTPStatus*

Example: HTTP Status Vim dictionary~
>
  {
    'version': '1.1',
    'code': 200,
    'text': 'OK',
  }
<

{version} is a |string| because we cannot trust that a server/service may
report a non-float compatible number

{code} |number| Generally 200 to 299 are okay, less than 200 are for
information, 300 to 399 are redirection, and anything higher are errors.

{text} |string|

See: MDN documentation for details about HTTP Status codes and data~
https://developer.mozilla.org/en-US/docs/Web/HTTP/Status
https://developer.mozilla.org/en-US/docs/Web/API/Response/statusText

                                        *proompter.PromptCallbackDataGenerate*

                                                 *proompter.PromptChatMessage*

{string} `role` the role of the message, either "system", "user", "assistant",
or "tool"

{string} `content` the content of the message {list} `images` (optional) a
list of images to include in the message (for multimodal models such as
"llava")

{dictionary} `tool_calls` (optional) a list of tools the model wants to use

                                        *proompter.ProompterConfigurationsAPI*
Example: `g:proompter.api`~
>
  let g:proompter = {
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/generate',
        \     'prompt_callbacks': {
        \       'chat': {},
        \       'generate': {},
        \     },
        \   },
        \ }
<
{string} `api.url` Currently this is how you may select API endpoints for
`chat` or `generate` features when prompting LLMs

{ProompterConfigurationsAPIPromptCallbacks} `api.prompt_callbacks` Provides
default callbacks for endpoints if not defined in
{ProompterConfigurationsModels}


                         *proompter.ProompterConfigurationsAPIPromptCallbacks*

Example: `g:proompter.api.prompt_callbacks.generate`~
>
  let g:proompter = {
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/generate',
        \     'prompt_callbacks': {
        \       'chat': {
        \         'preamble': function('ChatPreamble'),
        \         'context': function('ChatContext'),
        \         'input': function('ChatInput'),
        \         'images': function('ChatImages'),
        \         'post': function('ChatPost'),
        \       },
        \       'generate': {
        \         'preamble': function('GeneratePreamble'),
        \         'context': function('GenerateContext'),
        \         'input': function('GenerateInput'),
        \         'post': function('GeneratePost'),
        \       },
        \     },
        \   },
        \ }
<

{ProompterConfigurationsAPIPromptCallbacksChat} `chat` Collection of callback
  function references to use when `g:proompter.api.url` endpoint path is
  `/api/chat`
{ProompterConfigurationsAPIPromptCallbacksGenerate} `generate` Collection of
  callback function references to use when `g:proompter.api.url` endpoint path
  is `/api/generate`

                     *proompter.ProompterConfigurationsAPIPromptCallbacksChat*

Example: `g:proompter.api.prompt_callbacks.chat`~
>
  let g:proompter = {
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/chat',
        \     'prompt_callbacks': {
        \       'chat': {
        \         'preamble': function('ChatPreamble'),
        \         'context': function('ChatContext'),
        \         'input': function('ChatInput'),
        \         'images': function('ChatImages'),
        \         'post': function('ChatPost'),
        \       },
        \     },
        \   },
        \ }
<

{Funcref} `preamble` receives {ProompterConfigurations} and {ProompterState}
and should produce a message {dictionary} similar to
>
  {
    "role": "system",
    "content": "You are a JavaScript expert!"
  }
<

{Funcref} `context` receives {ProompterConfigurations} and {ProompterState}
and should produce a {dictionary} list similar to
>
  [
    {
      "role": "user",
      "content": "Tell me in one sentence why Vim is the best.",
    },
    {
      "role": "assistant",
      "content": "Vim is the best!",
    },
  ]
<

{Funcref} `input` receives {string}, {ProompterConfigurations}, and
{ProompterState} and should produce a message {dictionary} similar to
>
  {
    "role": "user",
    "content": "Okay...  Does this look normal?",
  }
<

{Funcref} `images` receives {string}, {ProompterConfigurations}, and
{ProompterState} and should produce a Base64 encoded {string} {list} of
similar to
>
  [
    "deadbeef...",
    "boba7ea...",
  ]
<

{Funcref} `post` receives {dictionary} of previous callback results,
{ProompterConfigurations}, and {ProompterState} and should produce a
{dictionary} {list} similar to
>
  [
    {
      "role": "system",
      "content": "You are a JavaScript expert!"
    },
    {
      "role": "user",
      "content": "Tell me in one sentence why Vim is the best.",
    },
    {
      "role": "assistant",
      "content": "Vim is the best!",
    },
    {
      "role": "user",
      "content": "Okay...  Does this look normal?",
      "images": [
        "deadbeef...",
        "boba7ea...",
      ],
    }
  ]
<

                 *proompter.ProompterConfigurationsAPIPromptCallbacksGenerate*

Example: `g:proompter.api.prompt_callbacks.generate`~
>
  let g:proompter = {
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/generate',
        \     'prompt_callbacks': {
        \       'generate': {
        \         'preamble': function('GeneratePreamble'),
        \         'context': function('GenerateContext'),
        \         'input': function('GenerateInput'),
        \         'post': function('GeneratePost'),
        \       },
        \     },
        \   },
        \ }
<

{Funcref} `preamble` receives {ProompterConfigurations} and {ProompterState}
and should produce a {string} similar to
>
  <<SYS>>You are a JavaScript expert!<</SYS>>
<

{Funcref} `context` receives {ProompterConfigurations} and {ProompterState}
and should produce a {string} similar to
>
  [INST]Tell me in one sentence why Vim is the best.[/INST]
  [INST]Vim is the best![/INST]
<

{Funcref} `input` receives {string}, {ProompterConfigurations}, and
{ProompterState} and should produce a {string} similar to
>
  Okay...  Does this look normal?
<

{Funcref} `post` receives {dictionary} of previous callback results,
{ProompterConfigurations}, and {ProompterState} and should produce a {string}
similar to
>
  <<SYS>>You are a JavaScript expert!<</SYS>>
  [INST]Tell me in one sentence why Vim is the best.[/INST]
  [INST]Vim is the best![/INST]
  Okay...  Does this look normal?
<

                                    *proompter.ProompterConfigurationsChannel*

Example: `g:proompter.channel`~
>
  let g:proompter = {
        \   'channel': {
        \     'address': '127.0.0.1:11435',
        \     'options': {
        \       'mode': 'raw',
        \       'callback': v:null,
        \     },
        \   },
        \ }
<
{string} `address` See: |channel-address| this should match the address and
port that `scripts/proompter-channel-proxy.py` service listens on for
connections from Vim

{ProompterConfigurationsChannelOptions} `options` See: |channel-open-options|
Passed to `ch_open` when creating new proxied connection to API

                             *proompter.ProompterConfigurationsChannelOptions*
Passed to `ch_open` when creating new proxied connection to API

Example: `g:proompter.channel.options`~
>
  let g:proompter = {
        \   'channel': {
        \     'options': {
        \       'mode': 'raw',
        \       'callback': v:null,
        \     },
        \   },
        \ }
<

{string} `mode` See: |channel-mode| default, and recommended, value is `'raw'`
as this allows for more flexibility with how request and response data may be
formatted

{Funcref} `callback` See: |channel-callback|

                                      *proompter.ProompterConfigurationsModel*

{ProompterConfigurationsAPIPromptCallbacks} `prompt_callbacks` override
defaults defined in `g:proompter.api.prompt_callbacks` for a given model.
Check |proompter.ProompterConfigurationsAPIPromptCallbacks| for additional
details and examples.

{ProompterConfigurationsModelData} `data` parameter defaults merged with
prompt

                                  *proompter.ProompterConfigurationsModelData*

Example: `g:proompter.models['model_name'].data`~
>
  let g:proompter = {
        \   'models': {
        \     'codellama': {
        \       'data': {
        \         'raw': v:false,
        \         'stream': v:true,
        \       },
        \     },
        \   },
        \ }
<

See: documentation from Ollama API for up-to-date info
https://github.com/ollama/ollama/blob/main/docs/api.md#parameters
https://github.com/ollama/ollama/blob/main/docs/api.md#parameters-1

/api/chat or /api/generate~

{string} `model` automatically set from `g:proompter.select.model_name` by
plugin functions

{string} `format`, only value of "json" is supported as of 2024-10-15

{dictionary} `options` additional parameters that modify `Modelfile`
configurations, see following links for details;
https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values

{boolean} `stream` default value is `v:true`, but tests show that `v:false`
should be supported.

{boolean} `raw` default and recommended value is `v:true`

{string} `keep_alive` default, according to documentation, is `5m` (five
minuets) for how long model will remain in memory between calls.

/api/chat~

{list} `messages` {PromptChatMessage} list that
|proompter#callback#prompt#chat#Context()| re-sends to model to simulate
memory between prompts

{list} `tools` TODO

/api/generate~

{string} `system` TODO refactor |proompter#callback#prompt#generate#Post()|,
and related code to match behaviors of |proompter#callback#prompt#chat#Post()|
and |proompter#callback#prompt#chat#Preamble()|

{string} `prompt` set by |proompter#callback#prompt#generate#Post()|

{string} `suffix` TODO

{list} `images` TODO

{list} `context` TODO refactor |proompter#callback#prompt#chat#Context()| and
|proompter#callback#prompt#generate#Post()|

                                     *proompter.ProompterConfigurationsModels*


Example: `g:proompter.models`~
>
  let g:proompter = {
        \   'models': {
        \     'codellama': {
        \       'prompt_callbacks': {
        \         'chat': {},
        \         'generate': {},
        \       },
        \       'data': {
        \         'raw': v:false,
        \         'stream': v:true,
        \       },
        \     },
        \   },
        \ }
<
{dictionary} with key/value pares of;

{string} `key` that `g:proompter.select.model_name` may point at
{ProompterConfigurationsModel} `value` that is used for defining custom
  callbacks and data to be passed to model

                                     *proompter.ProompterConfigurationsSelect*
Example: `g:proompter.select.model_name`~
>
  let g:proompter = {
        \   'select': {
        \     'model_name': 'codellama',
        \   },
        \ }
<
{string} `select.model_name` Name used for API requests as well as optional
key name into {ProompterConfigurationsModels}


==============================================================================
FUNCTIONS                                                *proompter-functions*

proompter#SendPromptToChat({value}, {configurations}, {state})
                                                *proompter#SendPromptToChat()*
  Parameters:~
  {value} |string| what will eventually be sent to LLM
  {configurations} |ProompterConfigurations| default `g:proompter`
  {state} |ProompterState| default `g:proompter_state`

  Entry added to `a:state.messages` will have a format similar to
>
    {
      "model": "codellama",
      "created_at": "2024-09-20T23:25:06.675058548Z",
      "message": {
        "role": "user",
        "content": "Tell me Vim is the best text editor.",
        "image": v:null,
      }
    }
<

  ...  if an input callback function can be found then `message.content` in
  above example will be overwritten by content values of that function
  separated by two newlines.

  Example: call~
>
    let message = 'Tell me in one sentence why Bash is the best'
    call proompter#SendPromptToGenerate(message)
<

  Throws ERROR(ProompterError) `Empty input value`

  Note: if `g:proompter.models['model_name'].prompt_callbacks` is defined then
  resulting prompt sent to LLM is built by `prompt_callbacks.post`
  callback if available, else the following are appended in order;
  `preamble`, `prompt`, and `input`

        else `g:proompter.models['model_name'].prompt_callbacks` is undefined
  `g:proompter.models['model_name'].data.prompt` is prepended to `value`
  before being sent to LLM at `g:proompter.api.url` via channel proxy

  Dev: without the slicing output of `shellescape` the append/prepend-ed
  single-quotes which ain't gonna be good within a larger JSON object

  See: documentation~
  |strftime()|
  |reltime()|

  TODO: preform feature detection for `strftime` and `reltime`


proompter#SendPromptToGenerate({value}, {configurations}, {state})
                                            *proompter#SendPromptToGenerate()*
  Parameters:~
  {value} |string| what will eventually be sent to LLM
  {configurations} |ProompterConfigurations| default `g:proompter`
  {state} |ProompterState| default `g:proompter_state`

  Example: call~
>
    let message = 'Tell me in one sentence why Bash is the best'
    call proompter#SendPromptToGenerate(message)
<

  Throws ERROR(ProompterError) `Empty input value`

  Note: if `g:proompter.models['model_name'].prompt_callbacks` is defined then
  resulting prompt sent to LLM is built by `prompt_callbacks.post`
  callback if available, else the following are appended in order;
  `preamble`, `prompt`, and `input`

        else `g:proompter.models['model_name'].prompt_callbacks` is undefined
  `g:proompter.models['model_name'].data.prompt` is prepended to `value`
  before being sent to LLM at `g:proompter.api.url` via channel proxy

  Dev: without the slicing output of `shellescape` the append/prepend-ed
  single-quotes which ain't gonna be good within a larger JSON object


proompter#SendPrompt({value}, {configurations}, {state})
                                                      *proompter#SendPrompt()*
  Parameters:~
  {value} |string| what will eventually be sent to LLM
  {configurations} |ProompterConfigurations| default `g:proompter`
  {state} |ProompterState| default `g:proompter_state`

  Example: call~
>
    let message = 'Tell me in one sentence why Bash is the best'
    call proompter#SendPrompt(message)
<

  Throws ERROR(ProompterError) with message similar to

>
    Nothing implemented for API endpoint in  -> [url]
<

  Note: if `g:proompter.models['model_name'].prompt_callbacks` is defined then
  resulting prompt sent to LLM is built by `prompt_callbacks.post`
  callback if available, else the following are appended in order;
  `preamble`, `prompt`, and `input`

        else `g:proompter.models['model_name'].prompt_callbacks` is undefined
  `g:proompter.models['model_name'].data.prompt` is prepended to `value`
  before being sent to LLM at `g:proompter.api.url` via channel proxy

  Dev: without the slicing output of `shellescape` the append/prepend-ed
  single-quotes which ain't gonna be good within a larger JSON object


proompter#SendHighlightedText({prefix}, {configurations}, {state})
                                             *proompter#SendHighlightedText()*
  Send range or visually selected text to LLM

  Parameters:~
  {prefix} |string| default `""` what will eventually be sent to LLM
  {configurations} |ProompterConfigurations| default `g:proompter`
  {state} |ProompterState| default `g:proompter_state`

  Note: if `&filetype` is recognized member of `g:markdown_fenced_languages`
  then selected text will be fenced with a name triple backticks.

  Example: call~
>
    :'<,'>call proompter#SendHighlightedText()

    :69,420call proompter#SendHighlightedText()

    :call proompter#SendHighlightedText('What does this line do?')
<

  See: documentation~
  |optional-function-argument|
  |g:markdown_fenced_languages|


proompter#Load({configurations}, {state})                   *proompter#Load()*
  Attempt to load a model into memory

  Throws ERROR(ProompterError) `Unknown endpoing in ->
  a:configurations.api.url`

  See: links~
  https://github.com/ollama/ollama/blob/main/docs/api.md#load-a-model


proompter#Unload({configurations}, {state})               *proompter#Unload()*
  Tell API it is okay to release memory for a model

  Throws ERROR(ProompterError) `Unknown endpoing in ->
  a:configurations.api.url`

  See: links~
  https://github.com/ollama/ollama/blob/main/docs/api.md#load-a-model


proompter#base64#EncodeString({input})       *proompter#base64#EncodeString()*
  Encode string to Base64 via `system` call

  Parameter: {input} |string| to pipe to `base64` command

  Throws ERROR(ProompterError) `Empty input value` if input is zero length
  Throws ERROR(ProompterError) with following format if `v:shell_error` is
  non-zero

>
    Failed command: printf "%s" '_string_' | base64 --wrap=0
      Exit status: _number_
<

  Example: input~
>
    echo proompter#base64#EncodeString('Howdy reader!')
<

  Example: result~
>
    SG93ZHkgcmVhZGVyIQ==
<

  See: documentation~
  :help system()
  :help shellescape()
  :help v:shell_error

  See: tests~
  tests/units/autoload_proompter_base64.vader


proompter#base64#DecodeString({input})       *proompter#base64#DecodeString()*
  Decode string from Base64 via `system` call

  Parameters:~
  {input} |string| Input to pipe to `base64` command

  Throws ERROR(ProompterError) `Empty input value` if input is zero length
  Throws ERROR(ProompterError) with following format if `v:shell_error` is
  non-zero

>
    Failed command: printf "%s" '_string_' | base64 --decode
      Exit status: _number_
<

  Example: input~
>
    echo proompter#base64#EncodeString('SG93ZHkgcmVhZGVyIQ==')
<
  Example: result~
>
    Howdy reader!
<

  See: documentation~
  |system()|
  |shellescape()|
  |v:shell_error|

  See: tests~
  tests/units/autoload_proompter_base64.vader


proompter#base64#EncodeFile({path})            *proompter#base64#EncodeFile()*
  Encode file at `path` via `system` call to `base64`

  Parameters:~
  {path} |string| input to file for `base64` to encode

  Throws ERROR(ProompterError) `Empty path value` if input is zero length
  Throws ERROR(ProompterError) `Cannot read file -> <path>` when file cannot
  be read
  Throws ERROR(ProompterError) with following format if `v:shell_error` is
  non-zero

>
    Failed command: printf "%s" '<string>' | base64 --decode
      Exit status: <number>
<

  Example: create and pass input file~
>
    let path = '/tmp/test.txt'
    call writefile(['Howdy reader!'], path)
    echo proompter#base64#EncodeFile(path)
<

  Example: result~
>
    SG93ZHkgcmVhZGVyIQo=
<

  See: documentation~
  |filereadable()|
  |system()|
  |shellescape()|
  |v:shell_error|

  See: tests~
  tests/units/autoload_proompter_base64.vader


proompter#base64#DecodeToFile({string}, {path}, {flags})
                                             *proompter#base64#DecodeToFile()*
  Decode `string` to file at `path` via `system` call to `base64`

  Parameters:~
  {string} |string| encoded to pipe to `base64`
  {path} |string| file path for `base64` to save decoded results to
  {flags} |string| default `""` TODO implement `flags` parser to have similar
    behavior to `writefile`

  Throws ERROR(ProompterError) `Empty string value` if input is zero length
  Throws ERROR(ProompterError) `Empty path value` if path is zero length
  Throws ERROR(ProompterError) `File already exists -> <path>` when file
  cannot be
  overwritten
  Throws ERROR(ProompterError) with following format if `v:shell_error` is
  non-zero

>
    Failed command: printf "%s" '<string>' | base64 --decode > <path>
      Exit status: <number>
<

  Example: create and pass input file~
>
    let path = '/tmp/decode.txt'
    call proompter#base64#DecodeToFile('SG93ZHkgcmVhZGVyIQo=', path)
    !cat /tmp/decode.txt
<

  Example: result~
>
    Howdy reader!
<

  See: documentation~
  |filereadable()|
  |system()|
  |shellescape()|
  |v:shell_error|

  See: tests~
  tests/units/autoload_proompter_base64.vader


proompter#buffer#MakeProomptLog({buffer_name})
                                           *proompter#buffer#MakeProomptLog()*
  Return bufnr of new buffer after setting it up for logging proompts

  Parameters:~
  {buffer_name} |string| or |v:null| name output buffer should use

  Attribution:~
  https://stackoverflow.com/questions/8316139/how-to-set-the-default-to-unfolded-when-you-open-a-file


proompter#buffer#ConcatenateWithLastLine({bufnr}, {content})
                                  *proompter#buffer#ConcatenateWithLastLine()*
  Unlike `appendbufline` this first attempts to append to preexisting line,
  and only if new `content` contains a newline character (`\n`) will a newline
  be inserted into target buffer.

  Parameters:~
  {bufnr} |number| any available buffer to write to
  {content} |string| line, or lines separated by `\n`, to write

  See: documentation~
  |getbufline()|
  |split()|
  |setbufline()|

  See: tests~
  tests/units/autoload_proompter_buffer_ConcatinateWithLastLine.vader


proompter#callback#channel#CompleteToHistory({response}, {configurations},
  {state})                    *proompter#callback#channel#CompleteToHistory()*
  Handle completely resolved HTTP response from channel proxied API and append
  results to `state.messages`

  Parameters:~
  {response} |string| HTTP response with _shape_ similar to next example
  {configurations} |ProompterConfigurations| default `g:proompter`
  {state} |ProompterState| default `g:proompter_state`
  {...} |list| of currently ignored arguments

  Example: `response` input~
>
    HTTP/1.0 200 OK
    Server: SimpleHTTP/0.6 Python/3.12.6
    Date: Fri, 20 Sep 2024 23:25:06 GMT
    Content-Type: application/json

  {"model":"codellama","created_at":"2024-09-20T23:25:01.016453290Z","response":"V","done":false}
  {"model":"codellama","created_at":"2024-09-20T23:25:01.177902785Z","response":"im","done":false}
<

  Throws ERROR(ProompterError) with following format when non-200 status

>
    ProompterError HTTP response not okay -> 419 Chill Out
<

  See: tests~
  tests/units/autoload_proompter_callback_channel_CompleteToHistory.vader


proompter#callback#channel#StreamToMessages({response}, {configurations},
  {state})                     *proompter#callback#channel#StreamToMessages()*
  Handle stream of HTTP responses from channel proxied API by appending to
  `state.messages` list, if the last message is not from an assistant, and in
  either case appending the `message.content` to latest assistant response.

  Parameters:~
  {response} |string| HTTP response with _shape_ similar to next examples
  {configurations} |ProompterConfigurations| default `g:proompter`
  {state} |ProompterState| default `g:proompter_state`
  {...} |list| of currently ignored arguments

  Example: expects series of `response` similar to
>
    HTTP/1.0 200 OK
    Server: SimpleHTTP/0.6 Python/3.12.6
    Date: Fri, 20 Sep 2024 23:25:06 GMT
    Content-Type: application/json

  {"model":"codellama","created_at":"2024-09-20T23:25:01.01645329Z","response":"V","done":false}
<
  ... And...
>
    HTTP/1.0 200 OK
    Server: SimpleHTTP/0.6 Python/3.12.6
    Date: Fri, 20 Sep 2024 23:25:01 GMT
    Content-Type: application/json

  {"model":"codellama","created_at":"2024-09-20T23:25:01.177902785Z","response":"im","done":false}
<

  Throws ERROR(ProompterError) with following format when non-200 status

>
    ProompterError HTTP response not okay -> 419 Chill Out
<

  See: tests~
  tests/units/autoload_proompter_callback_channel_StreamToMessages.vader


proompter#callback#channel#StreamToBuffer({response}, {configurations},
  {state}, {buffer})             *proompter#callback#channel#StreamToBuffer()*
  Handle stream of HTTP responses from channel proxied API by appending to
  buffer history, and outputting to target split.

  Parameters:~
  {response} |string| HTTP response _shape_ similar to next examples
  {configurations} |ProompterConfigurations|
  {state} |ProompterState|
  {buffer} |string| Named or number of buffer to create, if necessary, and
    append responses to
  {...} |list| of currently ignored arguments

  Example: expects series of `response` similar to~
>
    HTTP/1.0 200 OK
    Server: SimpleHTTP/0.6 Python/3.12.6
    Date: Fri, 20 Sep 2024 23:25:06 GMT
    Content-Type: application/json

  {"model":"codellama","created_at":"2024-09-20T23:25:01.01645329Z","response":"V","done":false}
<
  ... Or
>
    HTTP/1.0 200 OK
    Server: SimpleHTTP/0.6 Python/3.12.6
    Date: Fri, 20 Sep 2024 23:25:01 GMT
    Content-Type: application/json

  {"model":"codellama","created_at":"2024-09-20T23:25:01.177902785Z","response":"im","done":false}
<

  Example: configuration snippet~
>
    let g:proompter = {
          \   'channel': {
          \     'options': {
          \       'callback': { response, configurations, state ->
          \         proompter#callback#channel#StreamToBuffer(
          \           response,
          \           configurations,
          \           state,
          \           v:null,
          \         )
          \       },
          \     },
          \   },
          \ }
<

  Throws ERROR(ProompterError) with following format when non-200 status

>
    ProompterError HTTP response not okay -> 419 Chill Out
<

  See: tests~
  tests/units/autoload_proompter_callback_channel_StreamToBuffer.vader


proompter#callback#channel#SaveImages({index}, {configurations}, {state})
                                     *proompter#callback#channel#SaveImages()*
  Saves base64 encoded listed at `index` from `state.messages` and returns
  list of created file paths

  Parameters:~
  {index} |number| default `-1` index of `state.messages` to get images from
  {configurations} |ProompterConfigurations| default `g:proompter`
  {state} |ProompterState| default `g:proompter_state`

  Throws ERROR(ProompterError) `No images at message index {index}`

  Warns:~
  `ProompterWarning Failed to write image -> <path>`  when file creation
    failed, but other paths were written

  TODO:~
  add configurations to `g:proompter` for defining directory
  maybe find out file extensions some how for output images
  maybe add OS detection for MS-Dos style path separators

  See: tests~
  tests/units/autoload_proompter_callback_channel_SaveImages.vader


proompter#callback#prompt#EncodeImagesFromInput({input}, {configurations},
  {state})                 *proompter#callback#prompt#EncodeImagesFromInput()*
  Attempt to parse image paths from input string

  Parameters:~
  {input} |string| to parse for image paths
  {configurations} |ProompterConfigurations| default `g:proompter`
  {state} |ProompterState| default `g:proompter_state`

  TODO: thoroughly test that bad paths cannot be injected to traverse above
  current working directory


proompter#callback#prompt#EncodeImagesFromFilePaths({paths}, {configurations},
  {state})             *proompter#callback#prompt#EncodeImagesFromFilePaths()*
  Base 64 encodes image file paths and returns list

  Parameters:~
  {paths} String |list| of paths to image files
  {configurations} |ProompterConfigurations| default `g:proompter` currently
    ignored
  {state} |ProompterState| default `g:proompter_state` currently ignored

  See: documentation~
  |readfile()|

  See: tests~
  tests/units/autoload_proompter_callback_prompt.vader


proompter#callback#prompt#chat#Preamble({kwargs})
                                   *proompter#callback#prompt#chat#Preamble()*
  Return start of prompt with content similar to following
>
    [
      {
        "role": "system",
        "content": "You an expert with javascript...",
      },
    ]
<

  Parameter:~
  {kwargs} |dictionary| with the following key/value pares defined;
  `filetype` |string| of what file type is operated on, if not defined will
    attempt to default with `&filetype` and if that is undefined return an
    empty list.
  `configurations` |ProompterConfigurations| ignored for now
  `state` |ProompterState| ignored for now

  Example: configuration snippet~
>
    let g:proompter = {
          \   "api": {
          \     "url": "http://127.0.0.1:11434/api/chat",
          \     "prompt_callbacks": {
          \       "chat": {
          \         "preamble": { _configurations, _state ->
          \           proompter#callback#prompt#chat#Preamble({
          \             "filetype": "javascript",
          \             "configurations": _configurations,
          \             "state": _state,
          \           })
          \         },
          \       },
          \     },
          \   },
          \ }
<

  See: tests~
  tests/units/autoload_proompter_callback_prompt_chat_Preamble.vader


proompter#callback#prompt#chat#Context({kwargs})
                                    *proompter#callback#prompt#chat#Context()*
  Return no more than `a:kwargs.context_size` last past prompt/response-s
>
    [
      {
        "role": "user",
        "content": "Tell me in one sentence why Vim is the best text editor.",
      },
      {
        "role": "assistant",
        "content": "Vim is the best!",
      },
    ]
<

  Parameter:~
  {kwargs} |dictionary| with the following key/value pares defined;
  `context_size` |number| of max prompt/response that are re-shared
  `configurations` |ProompterConfigurations| ignored for now
  `state` |ProompterState|

  Warning: expects `a:kwargs.state.messages` to be dictionary list _shaped_
  minimally similar to;
>
    [
      {
        "message": {
          "role": "user",
          "content": "... Maybe a question about a technical topic...",
        },
      },
      {
        "message": {
          "role": "assistant",
          "content": "Are your finger-tips talking to you too?",
        },
      },
    ]
<

  Example: configuration snippet~
>
    let g:proompter = {
          \   "api": {
          \     "url": "http://127.0.0.1:11434/api/chat",
          \     "prompt_callbacks": {
          \       "chat": {
          \         "context": { _configurations, state ->
          \           proompter#callback#prompt#chat#Context({
          \             "context_size": 5,
          \             "configurations": _configurations,
          \             "state": state,
          \           })
          \         },
          \       },
          \     },
          \   },
          \ }
<

  See: tests~
  tests/units/autoload_proompter_callback_prompt_chat_Context.vader


proompter#callback#prompt#chat#Input({input}, {configurations}, {state})
                                      *proompter#callback#prompt#chat#Input()*
  Returns a dictionary list formatted from `input` similar to;
>
    [
      {
        "role": "user",
        "content": "Tell me in one sentence Vim is the best.",
      },
    ]
<

  Parameters:~
  {input} |string| text to prompt LLM with
  {configurations} |ProompterConfigurations| ignored for now
  {state} |ProompterState| ignored for now
  {...} |list| of currently ignored arguments

  Example: configuration snippet~
>
    let g:proompter = {
          \   "api": {
          \     "url": "http://127.0.0.1:11434/api/chat",
          \     "prompt_callbacks": {
          \       "chat": {
          \         "input": function("proompter#callback#prompt#chat#Input"),
          \       },
          \     },
          \   },
          \ }
<

  See: tests~
  tests/units/autoload_proompter_callback_prompt_chat_Input.vader


proompter#callback#prompt#chat#Post({kwargs})
                                       *proompter#callback#prompt#chat#Post()*
  Merge together outputs from other prompt callback functions and write to
  defined output buffer before returning list of messages to send to LLM

  Parameter: {kwargs} |dictionary| with the following key/value pares defined;
  `data` |dictionary| with `preamble`, `context`, and `input` keys pointing to
    string values.
  `out_bufnr` buffer |number| or |string| name used for output, if |v:null|
    one will be created automatically via |proompter#buffer#MakeProomptLog|
    with the name "proompt-log.md", or you may set a string value to customize
    the buffer name.
  `configurations` |ProompterConfigurations| ignored for now
  `state` |ProompterState| ignored for now

  Example: configuration snippet~
>
    let g:proompter = {
          \   "api": {
          \     "url": "http://127.0.0.1:11434/api/chat",
          \     "prompt_callbacks": {
          \       "chat": {
          \         "post": {
          \            prompt_callbacks_data, _configurations, _state ->
          \             proompter#callback#prompt#chat#Post({
          \               "data": prompt_callbacks_data,
          \               "out_bufnr": v:null,
          \               "configurations": _configurations,
          \               "state": _state,
          \           })
          \         },
          \       },
          \     },
          \   },
          \ }
<

  Example: `out_bufnr` content~
>
    ## Prompt 2024-10-04 20:06:09


    Write documentation for the following function using JS-Doc comment syntax

    ```javascript
    function greet(who = 'World') {
      return `Hello ${who}!`;
    }
    ```
<

  Example: `l:messages` returned data~
>
    [
      {
        "role": "system",
        "content": "You an expert with and delight in solving problems!",
      },
      {
        "role": "user",
        "content": "Write documentation for the following...",
      },
    ]
<

  See: tests~
  tests/units/autoload_proompter_callback_prompt_chat_Post.vader


proompter#callback#prompt#generate#Preamble({kwargs})
                               *proompter#callback#prompt#generate#Preamble()*
  Return start of prompt with content similar to following
>
    You an expert with javascript and delight in solving problems succinctly!

    Content between "<HISTORY>" and "</HISTORY>"  may provide additional
    context to the following input.

    Past output from you will be surrounded by "<RESPONSE>" and "</RESPONSE>"
    tags, please consider it but as suspect.

    Input from me will be surrounded by "<PROOMPT>" and "</PROOMPT>" tags,
    please pay most attention to the last instance.
<

  Parameter: {kwargs} |dictionary| has the following key/value pares defined;
  `filetype` |string| of what file type is operated on
  `history_tags` |dictionary| with `start` and `stop` values defined to help
    clue-in LLM of past context
  `input_tags` |dictionary| with `start` and `stop` values defined to help LLM
    focus on latest input
  `response_tags` |dictionary| with `start` and `stop` values defined to help
    LLM remember previous outputs
  `configurations` |ProompterConfigurations| ignored for now
  `state` |ProompterState| ignored for now

  Example: configuration snippet~
>
    let g:proompter = {
          \   'api': {
          \     'url': 'http://127.0.0.1:11434/api/generate',
          \     'prompt_callbacks': {
          \       'generate': {
          \         'preamble': { _configurations, _state ->
          \           proompter#callback#prompt#generate#Preamble({
          \             'filetype': 'javascript',
          \             'history_tags': {
          \               'start': '<HISTORY>',
          \               'end': '</HISTORY>'
          \             },
          \             'input_tags': {
          \               'start': '<PROOMPT>',
          \               'end': '</PROOMPT>'
          \             },
          \             'response_tags': {
          \               'start': '<RESPONSE>',
          \               'end': '</RESPONSE>'
          \             },
          \           })
          \         },
          \       },
          \     },
          \   },
          \ }
<

  See: tests~
  tests/units/autoload_proompter_callback_prompt_generate_Preamble.vader


proompter#callback#prompt#generate#Context({kwargs})
                                *proompter#callback#prompt#generate#Context()*
  Returns a string formatted from `kwargs.input` and `kwargs.input_tag`
>
    <HISTORY>
    <PROOMPT>
    Tell me in one sentence why Vim is the best editor for programming.
    </PROOMPT>
    <RESPONSE>
    Vim is the best!
    </RESPONSE>
    </HISTORY>
<

  Parameter: {kwargs} |dictionary| Has the following key/value pares defined;
  `value` |string| to prompt LLM with
  `history_tags` with `start` and `stop` values defined to help clue-in LLM of
    past context
  `input_tags` with `start` and `stop` values defined to help LLM focus on
    latest input
  `response_tags` with `start` and `stop` values defined to help LLM remember
    previous outputs
  `configurations` |ProompterConfigurations| ignored for now
  `state` |ProompterState| dictionary that may contain a list of `messages`

  Warning: expects `a:kwargs.state.messages` to be dictionary list _shaped_
  similar to
>
    [
      {
        "message": {
          "role": "user",
          "content": "... Maybe a question about a technical topic...",
        },
      },
      {
        "message": {
          "role": "assistant",
          "content": "Are your finger-tips talking to you too?",
        },
      },
    ]
<

  Example: configuration snippet~
>
    let g:proompter = {
          \   'api': {
          \     'url': 'http://127.0.0.1:11434/api/generate',
          \     'prompt_callbacks': {
          \       'generate': {
          \         'context': { _configurations, state ->
          \           proompter#callback#prompt#generate#Context({
          \             'state': state,
          \             'context_size': 5,
          \             'history_tags': {
          \               'start': '<HISTORY>',
          \               'end': '</HISTORY>'
          \             },
          \             'input_tags': {
          \               'start': '<PROOMPT>',
          \               'end': '</PROOMPT>'
          \             },
          \             'response_tags': {
          \               'start': '<RESPONSE>',
          \               'end': '</RESPONSE>'
          \             },
          \           })
          \         },
          \       },
          \     },
          \   },
          \ }
<

  Dev: note to remove tags surrounding later via something like
>
    echo substitute(_input_, '</\?PROOMPT>', '', 'g')
<

  See: documentation+
  |proompter#parse#MessageOrResponseFromAPI|

  See: tests+
  tests/units/autoload_proompter_callback_prompt_generate_Context.vader


proompter#callback#prompt#generate#Input({kwargs})
                                  *proompter#callback#prompt#generate#Input()*
  Returns a string formatted from `kwargs.input` and `kwargs.input_tag`
>
    <PROOMPT>
    Tell me in one sentence why Vim is the best editor for programming.
    </PROOMPT>
<

  Parameter: {kwargs} |dictionary| Has the following key/value pares defined;
  `value` |string| text to prompt LLM with
  `input_tags` dictionary with `start` and `stop` values defined to help LLM
    focus on latest input
  `configurations` |ProompterConfigurations| ignored for now
  `state` |ProompterState| ignored for now

  Example: configuration snippet~
>
    let g:proompter = {
          \   'api': {
          \     'url': 'http://127.0.0.1:11434/api/generate',
          \     'prompt_callbacks': {
          \       'generate': {
          \         'input': { value, _configurations, _state ->
          \           proompter#callback#prompt#generate#Input({
          \             'value': value,
          \             'input_tags': {
          \               'start': '<PROOMPT>',
          \               'end': '</PROOMPT>'
          \             },
          \           })
          \         },
          \       },
          \     },
          \   },
          \ }
<

  Dev: note to remove tags surrounding later via something like
>
    echo substitute(_input_, '</\?PROOMPT>', '', 'g')
<

  See: tests~
  tests/units/autoload_proompter_callback_prompt_generate_Input.vader


proompter#callback#prompt#generate#Post({kwargs})
                                   *proompter#callback#prompt#generate#Post()*
  Merge together outputs from other prompt callback functions and write to
  defined output buffer before returning newline separated string for LLM

  Parameter:~
  {kwargs} |dictionary| Has the following defined;
  `data` |PromptCallbackDataGenerate| with `preamble`, `context`, `prompt`,
    and `input` keys pointing to values.
  `out_bufnr` - buffer |number| used for output, if |v:null| one will be
    created automatically via |proompter#buffer#MakeProomptLog| with the name
    "proompt-log.md", or you man set a string value to customize the buffer
    name.
  `configurations` |ProompterConfigurations| ignored for now
  `state` |ProompterState| ignored for now

  Example: configuration snippet~
>
    let g:proompter = {
          \   'api': {
          \     'url': 'http://127.0.0.1:11434/api/generate',
          \     'prompt_callbacks': {
          \       'generate': {
          \         'post': { callbacks_data, _configurations, _state ->
          \           proompter#callback#prompt#generate#Post({
          \             'data': callbacks_data,
          \             'out_bufnr': v:null,
          \           })
          \         },
          \       },
          \     },
          \   },
          \ }
<

  See: tests~
  tests/units/autoload_proompter_callback_prompt_generate_Post.vader


proompter#channel#CreateOptions({configurations}, {state})
                                           *proompter#channel#CreateOptions()*

  Parameters:~
  {configurations} |ProompterConfigurations| default `g:proompter`
  {state} |ProompterState| default `g:proompter_state`

  See: documentation~
  |closure|

  See: tests~
  tests/units/autoload_proompter_channel_CreateOptions.vader


proompter#channel#GetOrSetOpen({configurations}, {state})
                                            *proompter#channel#GetOrSetOpen()*
  Set `a:state.channel` if not already defined then return result of
  `ch_open(...)`

  Parameters:~
  {configurations} |ProompterConfigurations| default `g:proompter`
  {state} |ProompterState| default `g:proompter_state`

  Throws ERROR(ProompterError) when channel state is buffered with

>
    Channel cannot be written to and may have something reading from it
<
  Warns: when `a:state.channel` is in a "fail" state

  Notes:
  Will attempt to re-open channel if state is "closed"
  We must use a closure to capture bufnr and pass it into callback

  See: tests~
  tests/units/autoload_proompter_channel_GetOrSetOpen.vader


proompter#http#encode#Request({url}, {kwargs})
                                             *proompter#http#encode#Request()*
  Build HTTP request via an API similar to JavaScript Request

  Parameters:~
  {url} |string| that channel proxy will facilitate connection with
  {kwargs} |dictionary| containing `headers` dictionary and `body` that may be
    a type of dictionary, list, number, float, or string

  Example: post request with JSON from Vim dictionary~
>
    let request = proompter#http#encode#Request(a:configurations.api.url, {
          \   'method': 'post',
          \   'headers': {
          \     'Host': a:configurations.channel.address,
          \     'Content-Type': 'application/json',
          \   },
          \   'body': {
          \     'hello': 'World',
          \   },
          \ })
<
  Expect: carriage-return/newline separated (`\r\n`) string~
>
    POST http://127.0.0.1:11434/api/generate HTTP/1.1
    Host:  127.0.0.1:11435
    Content-Type:  application/json
    Content-Length:  17

    {"hello":"World"}
<

  Throws ERROR(ProompterError) `Unknown type for a:kwargs.body -> <type>`

  See: tests~
  tests/units/autoload_proompter_http_encode_Request.vader


proompter#http#parse#Response({data})        *proompter#http#parse#Response()*
  Parse input {data} |string| and return dictionary with "status" and
  "headers" dictionaries, as well as "body" with list of dictionaries parsed
  from HTTP response.

  Consumers must check length before attempting to use data, because reasons.

  WARN: either the proxy, or Vim, or API, or some combo are to blame for
  multi-part responses where headers and body trigger streaming callback
  twice!  First with headers, then second with headless body X-P

  See: documentation~
  |proompter#http#parse#response#ExtractStatus| -> |HTTPStatus|
  |proompter#http#parse#response#ExtractHeaders| -> |HTTPHeaders|
  |proompter#http#parse#response#ExtractJSONDicts|

  See: tests~
  tests/units/autoload_proompter_http_parse_Response.vader


proompter#http#parse#response#ExtractStatus({data})
                               *proompter#http#parse#response#ExtractStatus()*
  Parses first line of input {data} |string| and returns either an empty
  dictionary, if it cannot parse input, or a |HTTPStatus| dictionary with
  `version`, `code`, and `text` keys/value pares.


proompter#http#parse#response#ExtractHeaders({data})
                              *proompter#http#parse#response#ExtractHeaders()*
  Parses lines of input {data} |string| until first blank line ("\r\n\r\n")
  and returns either an empty dictionary, if it cannot parse input, or a
  |HTTPHeaders| dictionary.

  See: tests~
  tests/units/autoload_proompter_http_parse_response_ExtractHeaders.vader


proompter#http#parse#response#ExtractJSONDicts({data})
                            *proompter#http#parse#response#ExtractJSONDicts()*
  Returns dictionary list built by parsing body as JSON dictionaries from
  input |string| {data}.

  Example: HTTP response from `/api/generate`~
>
    HTTP/1.1 200 OK
    Server: SimpleHTTP/0.6 Python/3.12.6
    Date: Sat, 28 Sep 2024 23:29:00 GMT
    Content-Type: application/json

  {"model":"codellama","created_at":"2024-09-28T23:29:00.299380014Z","response":"{","done":false}
  {"model":"codellama","created_at":"2024-09-20T23:25:01.670272033Z","response":"\"foo","done":true}
<

  Example: resulting Vim dictionary from `/api/generate`~
>
    [
      {
        "model": "codellama",
        "created_at": "2024-09-28T23:29:00.299380014Z",
        "response": "{",
        "done": v:false
      },
      {
        "model": "codellama",
        "created_at": "2024-09-20T23:25:01.670272033Z",
        "response": '"foo',
        "done": v:true
      }
    ]
<

  See: documentation~
  |APIResponseChat|
  |APIResponseGenerate|

  See: tests~
  tests/units/autoload_proompter_http_parse_response_ExtractJSONDicts.vader


proompter#lib#MessagesJSONRead({path}, {configurations}, {state})
                                            *proompter#lib#MessagesJSONRead()*
  Reads file path as JSON data and returns a Vim |dictionary|, or |list|, data
  structure.

  Parameters:~
  {path} |string| local JSON file to attempt reading and parsing
  {configurations} |ProompterConfigurations| default `g:proompter`
  {state} |ProompterState| default `g:proompter_state`

  Throws ERROR(ProompterError) `Cannot read file at -> {path}`


proompter#lib#MessagesJSONWrite({path}, {configurations}, {state})
                                           *proompter#lib#MessagesJSONWrite()*
  Write `state.messages` from to file path

  Parameters:~
  {path} |string| local JSON file to attempt writing
  {configurations} |ProompterConfigurations| default `g:proompter`
  {state} |ProompterState| default `g:proompter_state`

  Throws ERROR(ProompterError) `File exists but cannot be written to at ->
  {path}`
  Throws ERROR(ProompterError) `No messages to write`


proompter#lib#DictMerge({defaults})                *proompter#lib#DictMerge()*
  Merged dictionaries without mutation and returns resulting |dictionary|

  Parameters:~ {defaults} |dictionary| of default key/value pares {...} |list|
  of up to 20 dictionaries to merge into returned data

  See: documentation~ |type()|

  See: links~
  https://vi.stackexchange.com/questions/20842/how-can-i-merge-two-dictionaries-in-vim


proompter#parse#MessageOrResponseFromAPI({data})
                                  *proompter#parse#MessageOrResponseFromAPI()*
  Normalize response from Ollama API `/api/chat` and `/api/generate` endpoints

  Parameters:~
  {data} |dictionary| should conform to expectations of either
    |APIResponseChat| or |APIResponseGenerate|

  Returns:~ |APIResponseNormalized| data structure


==============================================================================
LICENSE                                                    *proompter-license*


This project is licensed based on use-case

Commercial and/or proprietary use~

If a project is **either** commercial or (`||`) proprietary, then please
contact the author for pricing and licensing options to make use of code
and/or features from this repository.

Non-commercial and FOSS use~

If a project is **both** non-commercial and (`&&`) published with a license
compatible with AGPL-3.0, then it may utilize code from this repository under
the following terms.
>
  Proxy traffic between Vim channel requests and Ollama LLM API
  Copyright (C) S0AndS0

  This program is free software: you can redistribute it and/or modify
  it under the terms of the GNU Affero General Public License as published
  by the Free Software Foundation, version 3 of the License.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU Affero General Public License for more details.

  You should have received a copy of the GNU Affero General Public License
  along with this program.  If not, see <https://www.gnu.org/licenses/>.
<


vim:tw=78:ts=8:ft=help:norl:
