
""
" {{{
Execute (proompter#SendPromptToGenerate -- Uses default channel callback when streaming):
  let response_data = [
        \   {
        \     'model': 'codellama',
        \     'created_at': '2024-09-28T23:29:04.299380014Z',
        \     'response': 'This is a mock chat streaming response from the proxy',
        \     'done': v:false,
        \   },
        \   {
        \     'model': 'codellama',
        \     'created_at': '2024-09-28T23:29:04.299380014Z',
        \     'response': '!',
        \     'done': v:true,
        \     'context': ['1', '2', '3'],
        \     'total_duration': 8113331500,
        \     'load_duration': 6396458,
        \     'prompt_eval_count': 61,
        \     'prompt_eval_duration': 398801000,
        \     'eval_count': 4681,
        \     'eval_duration': 7701267000
        \   },
        \ ]

  let base64_encoded_response = proompter#base64#EncodeString(join([
        \   'HTTP/1.1 200 OK',
        \   'Server: SimpleHTTP/0.6 Python/3.12.6',
        \   'Date: ' . strftime('%a, %d %b %Y %T GMT'),
        \   'Content-Type: application/json',
        \   '',
        \   join(mapnew(response_data, { _index, entry -> json_encode(entry) }), "\r\n"),
        \ ], "\r\n"))

  let configurations = {
        \   'select': {
        \     'model_name': 'codellama',
        \     'completion_endpoint': 'generate',
        \   },
        \   'api': {
        \     'url': 'http://127.0.0.1:11434?response=' . base64_encoded_response,
        \   },
        \   'channel': {
        \     'address': '127.0.0.1:41968',
        \     'options': {
        \       'mode': 'raw',
        \       'callback': v:null,
        \     },
        \   },
        \   'models': {
        \     'codellama': {
        \       'data': {
        \         'raw': v:false,
        \         'stream': v:true,
        \       },
        \     },
        \   },
        \ }

  let state = {
        \   'messages': [],
        \   'channel': v:null,
        \ }

  let expected_configurations = deepcopy(configurations)

  let expected_state = deepcopy(state)

  let expected_entry = {
        \   'model': configurations.select.model_name,
        \   'created_at': response_data[-1].created_at,
        \   'done': response_data[-1].done,
        \   'done_reason': v:null,
        \   'context': response_data[-1].context,
        \   'message': {
        \     'role': 'assistant',
        \     'content': join(mapnew(response_data, { _index, entry ->
        \       entry.response
        \     }), ''),
        \     'images': v:null,
        \     'tool_calls': v:null,
        \   },
        \ }
  call add(expected_state.messages, expected_entry)

  try
    let value = 'This is a streaming request from Vim!'

    call proompter#SendPromptToGenerate(
          \   value,
          \   configurations,
          \   state,
          \ )

    sleep 100m
  catch
    echow 'v:exception ->' v:exception
  finally
    if ch_info(state.channel).status != 'closed'
      call ch_close(state.channel)
    endif
  endtry

  AssertEqual state.messages[-1], expected_state.messages[-1]

  AssertEqual configurations, expected_configurations, 'Unexpected mutation of configurations'

Execute (proompter#SendPromptToGenerate -- Uses default channel callback when non-streaming):
  let response_data = [
        \   {
        \     'model': 'codellama',
        \     'created_at': '2024-09-28T23:29:04.299380014Z',
        \     'response': 'This is a mock chat streaming response from the proxy',
        \     'done': v:false,
        \   },
        \   {
        \     'model': 'codellama',
        \     'created_at': '2024-09-28T23:29:04.299380014Z',
        \     'response': '!',
        \     'done': v:true,
        \     'context': ['1', '2', '3'],
        \     'total_duration': 8113331500,
        \     'load_duration': 6396458,
        \     'prompt_eval_count': 61,
        \     'prompt_eval_duration': 398801000,
        \     'eval_count': 4681,
        \     'eval_duration': 7701267000
        \   },
        \ ]


  let base64_encoded_response = proompter#base64#EncodeString(join([
        \   'HTTP/1.1 200 OK',
        \   'Server: SimpleHTTP/0.6 Python/3.12.6',
        \   'Date: ' . strftime('%a, %d %b %Y %T GMT'),
        \   'Content-Type: application/json',
        \   '',
        \   join(mapnew(response_data, { _index, entry -> proompter#json#Stringify(entry) }), "\r\n"),
        \ ], "\r\n"))

  let configurations = {
        \   'select': {
        \     'model_name': 'codellama',
        \   },
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/generate?response=' . base64_encoded_response,
        \   },
        \   'channel': {
        \     'address': '127.0.0.1:41968',
        \     'options': {
        \       'mode': 'raw',
        \       'callback': v:null,
        \     },
        \   },
        \   'models': {
        \     'codellama': {
        \       'data': {
        \         'raw': v:false,
        \         'stream': v:false,
        \       },
        \     },
        \   },
        \ }

  let state = {
        \   'messages': [],
        \   'channel': v:null,
        \ }

  let expected_configurations = deepcopy(configurations)

  let expected_state = deepcopy(state)

  let expected_entry = {
        \   'model': configurations.select.model_name,
        \   'created_at': response_data[-1].created_at,
        \   'done': response_data[-1].done,
        \   'done_reason': v:null,
        \   'context': response_data[-1].context,
        \   'message': {
        \     'role': 'assistant',
        \     'content': join(mapnew(response_data, { _index, entry ->
        \       entry.response
        \     }), ''),
        \     'images': v:null,
        \     'tool_calls': v:null,
        \   },
        \ }
  call add(expected_state.messages, expected_entry)

  try
    let value = 'This is a streaming request from Vim!'

    call proompter#SendPromptToGenerate(
          \   value,
          \   configurations,
          \   state,
          \ )

    sleep 100m
  catch
    echow 'v:exception ->' v:exception
  finally
    if ch_info(state.channel).status != 'closed'
      call ch_close(state.channel)
    endif
  endtry

  AssertEqual state.messages[-1], expected_state.messages[-1]

  AssertEqual configurations, expected_configurations, 'Unexpected mutation of configurations'

Execute (proompter#SendPromptToGenerate -- Appends to buffer via example channel callback):
  let response_data = [
        \   {
        \     'model': 'codellama',
        \     'created_at': '2024-09-28T23:29:04.299380014Z',
        \     'response': "First line of mocked streaming response\n\nThird line of mocked streaming response!",
        \     'done': v:true,
        \     'context': ['1', '2', '3'],
        \     'total_duration': 8113331500,
        \     'load_duration': 6396458,
        \     'prompt_eval_count': 61,
        \     'prompt_eval_duration': 398801000,
        \     'eval_count': 46811,
        \     'eval_duration': 7701267000
        \   },
        \ ]


  let base64_encoded_response = proompter#base64#EncodeString(join([
        \   'HTTP/1.1 200 OK',
        \   'Server: SimpleHTTP/0.6 Python/3.12.6',
        \   'Date: ' . strftime('%a, %d %b %Y %T GMT'),
        \   'Content-Type: application/json',
        \   '',
        \   join(mapnew(response_data, { _index, entry -> proompter#json#Stringify(entry) }), "\r\n"),
        \ ], "\r\n"))

  let configurations = {
        \   'select': {
        \     'model_name': 'codellama',
        \   },
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/generate?response=' . base64_encoded_response,
        \   },
        \   'channel': {
        \     'address': '127.0.0.1:41968',
        \     'options': {
        \       'mode': 'raw',
        \       'callback': { _channel_response, api_response ->
        \         proompter#callback#channel#StreamToBuffer(
        \           api_response,
        \           g:configurations,
        \           g:state,
        \           '[Vader-workbench]',
        \         )
        \       },
        \     },
        \   },
        \   'models': {
        \     'codellama': {
        \       'data': {
        \         'raw': v:false,
        \         'stream': v:true,
        \       },
        \     },
        \   },
        \ }

  let state = {
        \   'messages': [],
        \   'channel': v:null,
        \ }

  let expected_configurations = deepcopy(configurations)

  let expected_state = deepcopy(state)

  let expected_entry = {
        \   'model': configurations.select.model_name,
        \   'created_at': response_data[-1].created_at,
        \   'done': response_data[-1].done,
        \   'done_reason': v:null,
        \   'context': response_data[-1].context,
        \   'message': {
        \     'role': 'assistant',
        \     'content': join(mapnew(response_data, { _index, entry ->
        \       entry.response
        \     }), ''),
        \     'images': v:null,
        \     'tool_calls': v:null,
        \   },
        \ }
  call add(expected_state.messages, expected_entry)

  try
    let value = 'This is a streaming request from Vim!'

    call proompter#SendPromptToGenerate(
          \   value,
          \   configurations,
          \   state,
          \ )

    sleep 100m
  catch
    echow 'v:exception ->' v:exception
  finally
    if ch_info(state.channel).status != 'closed'
      call ch_close(state.channel)
    endif
  endtry

  AssertEqual state.messages[-1], expected_state.messages[-1]
  AssertEqual configurations, expected_configurations, 'Unexpected mutation of configurations'
  AssertEqual getbufline(bufnr('[Vader-workbench]'), 0, '$')[-5:-3], split(response_data[0].response, '\n')

" }}}
""

