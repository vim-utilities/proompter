
""
" {{{
Execute (proompter#SendPrompt -- Correctly detects and uses /api/chat endpoint features):
  let response_data = [
        \   {
        \     'model': 'codellama',
        \     'created_at': '2024-09-28T23:29:04.299380014Z',
        \     'message': {
        \       'role': 'assistant',
        \       'content': 'This is a mock chat streaming response from the proxy',
        \       'images': v:null,
        \     },
        \     'done': v:false,
        \   },
        \   {
        \     'model': 'codellama',
        \     'created_at': '2024-09-28T23:29:04.299380014Z',
        \     'message': {
        \       'role': 'assistant',
        \       'content': '!',
        \       'images': v:null,
        \     },
        \     'done': v:true,
        \     'done_reason': 'stop',
        \     'total_duration': 8113331500,
        \     'load_duration': 6396458,
        \     'prompt_eval_count': 61,
        \     'prompt_eval_duration': 398801000,
        \     'eval_count': 468,
        \     'eval_duration': 7701267000
        \   },
        \ ]

  let base64_encoded_response = proompter#base64#EncodeString(join([
        \   'HTTP/1.1 200 OK',
        \   'Server: SimpleHTTP/0.6 Python/3.12.6',
        \   'Date: ' . strftime('%a, %d %b %Y %T GMT'),
        \   'Content-Type: application/json',
        \   '',
        \   join(mapnew(response_data, { _index, entry -> proompter#json#Stringify(entry) }), "\r\n"),
        \ ], "\r\n"))

  let configurations = {
        \   'select': {
        \     'model_name': 'codellama',
        \     'completion_endpoint': 'chat',
        \   },
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/chat?response=' . base64_encoded_response,
        \   },
        \   'channel': {
        \     'address': '127.0.0.1:41968',
        \     'options': {
        \       'mode': 'raw',
        \       'callback': v:null,
        \     },
        \   },
        \   'models': {
        \     'codellama': {
        \       'data': {
        \         'raw': v:false,
        \         'stream': v:true,
        \       },
        \     },
        \   },
        \ }

  let state = {
        \   'messages': [],
        \   'channel': v:null,
        \ }

  let expected_configurations = deepcopy(configurations)

  let expected_state = deepcopy(state)

  let expected_entry = {
        \   'model': configurations.select.model_name,
        \   'created_at': response_data[-1].created_at,
        \   'done': response_data[-1].done,
        \   'done_reason': response_data[-1].done_reason,
        \   'context': v:null,
        \   'message': {
        \     'role': response_data[-1].message.role,
        \     'content': join(mapnew(response_data, { _index, entry ->
        \       entry.message.content
        \     }), ''),
        \     'images': v:null,
        \     'tool_calls': v:null,
        \   },
        \ }
  call add(expected_state.messages, expected_entry)

  try
    let value = 'This is a streaming request from Vim!'

    call proompter#SendPrompt(
          \   value,
          \   configurations,
          \   state,
          \ )

    sleep 100m
  catch
    echow 'v:exception ->' v:exception
  finally
    if ch_info(state.channel).status != 'closed'
      call ch_close(state.channel)
    endif
  endtry

  AssertEqual state.messages[-1], expected_state.messages[-1]

  AssertEqual configurations, expected_configurations, 'Unexpected mutation of configurations'

Execute (proompter#SendPrompt -- Correctly detects and uses /api/generate endpoint features):
  let response_data = [
        \   {
        \     'model': 'codellama',
        \     'created_at': '2024-09-28T23:29:04.299380014Z',
        \     'response': 'This is a mock chat streaming response from the proxy',
        \     'done': v:false,
        \   },
        \   {
        \     'model': 'codellama',
        \     'created_at': '2024-09-28T23:29:04.299380014Z',
        \     'response': '!',
        \     'done': v:true,
        \     'context': ['1', '2', '3'],
        \     'total_duration': 8113331500,
        \     'load_duration': 6396458,
        \     'prompt_eval_count': 61,
        \     'prompt_eval_duration': 398801000,
        \     'eval_count': 468,
        \     'eval_duration': 7701267000,
        \     'TODO': 'REMOVE THIS.',
        \   },
        \ ]

  let base64_encoded_response = proompter#base64#EncodeString(join([
        \   'HTTP/1.1 200 OK',
        \   'Server: SimpleHTTP/0.6 Python/3.12.6',
        \   'Date: ' . strftime('%a, %d %b %Y %T GMT'),
        \   'Content-Type: application/json',
        \   '',
        \   join(mapnew(response_data, { _index, entry -> proompter#json#Stringify(entry) }), "\r\n"),
        \ ], "\r\n"))

  let configurations = {
        \   'select': {
        \     'model_name': 'codellama',
        \     'completion_endpoint': 'generate',
        \   },
        \   'api': {
        \     'url': 'http://127.0.0.1:11434/api/generate?response=' . base64_encoded_response,
        \   },
        \   'channel': {
        \     'address': '127.0.0.1:41968',
        \     'options': {
        \       'mode': 'raw',
        \       'callback': v:null,
        \     },
        \   },
        \   'models': {
        \     'codellama': {
        \       'data': {
        \         'raw': v:false,
        \         'stream': v:true,
        \       },
        \     },
        \   },
        \ }

  let state = {
        \   'messages': [],
        \   'channel': v:null,
        \ }

  let expected_configurations = deepcopy(configurations)

  let expected_state = deepcopy(state)

  let expected_entry = {
        \   'model': configurations.select.model_name,
        \   'created_at': response_data[-1].created_at,
        \   'done': response_data[-1].done,
        \   'done_reason': v:null,
        \   'context': response_data[-1].context,
        \   'message': {
        \     'role': 'assistant',
        \     'content': join(mapnew(response_data, { _index, entry ->
        \       entry.response
        \     }), ''),
        \     'images': v:null,
        \     'tool_calls': v:null,
        \   },
        \ }
  call add(expected_state.messages, expected_entry)

  try
    let value = 'This is a streaming request from Vim!'

    call proompter#SendPrompt(
          \   value,
          \   configurations,
          \   state,
          \ )

    sleep 100m
  catch
    echow 'v:exception ->' v:exception
  finally
    if ch_info(state.channel).status != 'closed'
      call ch_close(state.channel)
    endif
  endtry

  AssertEqual state.messages[-1], expected_state.messages[-1]

  AssertEqual configurations, expected_configurations, 'Unexpected mutation of configurations'
" }}}
""

""
" {{{
Execute (proompter#SendPrompt -- Throws when unknown completion_endpoint is configured):
  let configurations = {
        \   'select': {
        \     'model_name': 'mistral',
        \   },
        \ }

  let expected_configurations = deepcopy(configurations)

  let state = { 'messages': [] }

  let expected_state = { 'messages': [ ] }

  AssertThrows call proompter#SendPrompt('Wat', g:configurations, g:state)

  AssertEqual g:vader_exception, 'ProompterError Nothing implemented for API endpoint in  -> a:configurations.select.completion_endpoint'
  AssertEqual state, expected_state, 'Unexpected mutation of state'
  AssertEqual configurations, expected_configurations, 'Unexpected mutation of configurations'
" }}}
""

